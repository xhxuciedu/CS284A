{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xhxuciedu/CS175/blob/master/VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Nz0Kixr_9wUA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "import matplotlib.pylab as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IRf_9MbC9z_I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Create a directory if not exists\n",
        "sample_dir = 'samples'\n",
        "if not os.path.exists(sample_dir):\n",
        "    os.makedirs(sample_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bwoHIHQk93TX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Hyper-parameters\n",
        "image_size = 784\n",
        "h_dim = 400\n",
        "z_dim = 20\n",
        "num_epochs = 15\n",
        "batch_size = 128\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# MNIST dataset\n",
        "dataset = torchvision.datasets.MNIST(root='../../data',\n",
        "                                     train=True,\n",
        "                                     transform=transforms.ToTensor(),\n",
        "                                     download=True)\n",
        "\n",
        "# Data loader\n",
        "data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "06JzRR77958m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, image_size=784, h_dim=400, z_dim=20):\n",
        "        super(VAE, self).__init__()\n",
        "        self.fc1 = nn.Linear(image_size, h_dim)\n",
        "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
        "        self.fc3 = nn.Linear(h_dim, z_dim)\n",
        "        self.fc4 = nn.Linear(z_dim, h_dim)\n",
        "        self.fc5 = nn.Linear(h_dim, image_size)\n",
        "        \n",
        "    def encode(self, x):\n",
        "        h = F.relu(self.fc1(x))\n",
        "        return self.fc2(h), self.fc3(h)\n",
        "    \n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(log_var/2)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = F.relu(self.fc4(z))\n",
        "        return F.sigmoid(self.fc5(h))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        mu, log_var = self.encode(x)\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        x_reconst = self.decode(z)\n",
        "        return x_reconst, mu, log_var\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jfUmihSR9-Am",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = VAE().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xc_rKw7o-AiW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11815
        },
        "outputId": "59512f2c-8451-4330-f9b9-e45017c45908"
      },
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (x, _) in enumerate(data_loader):\n",
        "        # Forward pass\n",
        "        x = x.to(device).view(-1, image_size)\n",
        "        x_reconst, mu, log_var = model(x)\n",
        "        \n",
        "        # Compute reconstruction loss and kl divergence\n",
        "        # For KL divergence, see Appendix B in VAE paper or http://yunjey47.tistory.com/43\n",
        "        reconst_loss = F.binary_cross_entropy(x_reconst, x, size_average=False)\n",
        "        kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        \n",
        "        # Backprop and optimize\n",
        "        loss = reconst_loss + kl_div\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 10 == 0:\n",
        "            print (\"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}\" \n",
        "                   .format(epoch+1, num_epochs, i+1, len(data_loader), reconst_loss.item(), kl_div.item()))\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Save the sampled images\n",
        "        z = torch.randn(batch_size, z_dim).to(device)\n",
        "        out = model.decode(z).view(-1, 1, 28, 28)\n",
        "        save_image(out, os.path.join(sample_dir, 'sampled-{}.png'.format(epoch+1)))\n",
        "\n",
        "        # Save the reconstructed images\n",
        "        out, _, _ = model(x)\n",
        "        x_concat = torch.cat([x.view(-1, 1, 28, 28), out.view(-1, 1, 28, 28)], dim=3)\n",
        "        save_image(x_concat, os.path.join(sample_dir, 'reconst-{}.png'.format(epoch+1)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch[1/15], Step [10/469], Reconst Loss: 36018.1250, KL Div: 3316.1599\n",
            "Epoch[1/15], Step [20/469], Reconst Loss: 30197.2266, KL Div: 1074.8076\n",
            "Epoch[1/15], Step [30/469], Reconst Loss: 27008.5020, KL Div: 1051.5869\n",
            "Epoch[1/15], Step [40/469], Reconst Loss: 27106.0078, KL Div: 611.6204\n",
            "Epoch[1/15], Step [50/469], Reconst Loss: 25929.8789, KL Div: 774.0661\n",
            "Epoch[1/15], Step [60/469], Reconst Loss: 25534.3066, KL Div: 781.9841\n",
            "Epoch[1/15], Step [70/469], Reconst Loss: 25142.7656, KL Div: 867.3796\n",
            "Epoch[1/15], Step [80/469], Reconst Loss: 24148.2617, KL Div: 1093.0725\n",
            "Epoch[1/15], Step [90/469], Reconst Loss: 23668.8691, KL Div: 1198.8484\n",
            "Epoch[1/15], Step [100/469], Reconst Loss: 21401.8496, KL Div: 1392.6473\n",
            "Epoch[1/15], Step [110/469], Reconst Loss: 21778.2402, KL Div: 1422.3120\n",
            "Epoch[1/15], Step [120/469], Reconst Loss: 20727.0352, KL Div: 1690.9142\n",
            "Epoch[1/15], Step [130/469], Reconst Loss: 20078.6211, KL Div: 1640.8726\n",
            "Epoch[1/15], Step [140/469], Reconst Loss: 19479.4668, KL Div: 1707.6200\n",
            "Epoch[1/15], Step [150/469], Reconst Loss: 19652.3633, KL Div: 1738.0225\n",
            "Epoch[1/15], Step [160/469], Reconst Loss: 18893.1367, KL Div: 1763.1787\n",
            "Epoch[1/15], Step [170/469], Reconst Loss: 18275.4258, KL Div: 1838.0977\n",
            "Epoch[1/15], Step [180/469], Reconst Loss: 18663.0137, KL Div: 2002.9324\n",
            "Epoch[1/15], Step [190/469], Reconst Loss: 17855.4688, KL Div: 1985.9551\n",
            "Epoch[1/15], Step [200/469], Reconst Loss: 18396.0449, KL Div: 2046.4541\n",
            "Epoch[1/15], Step [210/469], Reconst Loss: 17442.8672, KL Div: 2066.8579\n",
            "Epoch[1/15], Step [220/469], Reconst Loss: 17213.0859, KL Div: 2161.0515\n",
            "Epoch[1/15], Step [230/469], Reconst Loss: 16560.5723, KL Div: 2197.6226\n",
            "Epoch[1/15], Step [240/469], Reconst Loss: 17740.6367, KL Div: 1999.9694\n",
            "Epoch[1/15], Step [250/469], Reconst Loss: 17192.7812, KL Div: 2078.7461\n",
            "Epoch[1/15], Step [260/469], Reconst Loss: 16860.2812, KL Div: 2185.5432\n",
            "Epoch[1/15], Step [270/469], Reconst Loss: 15671.1904, KL Div: 2229.7793\n",
            "Epoch[1/15], Step [280/469], Reconst Loss: 16325.4170, KL Div: 2093.1057\n",
            "Epoch[1/15], Step [290/469], Reconst Loss: 15701.9082, KL Div: 2308.9839\n",
            "Epoch[1/15], Step [300/469], Reconst Loss: 15181.9482, KL Div: 2255.8218\n",
            "Epoch[1/15], Step [310/469], Reconst Loss: 15861.6562, KL Div: 2373.8037\n",
            "Epoch[1/15], Step [320/469], Reconst Loss: 14788.7422, KL Div: 2265.1680\n",
            "Epoch[1/15], Step [330/469], Reconst Loss: 14518.1436, KL Div: 2587.7991\n",
            "Epoch[1/15], Step [340/469], Reconst Loss: 15052.9062, KL Div: 2245.6650\n",
            "Epoch[1/15], Step [350/469], Reconst Loss: 14689.5771, KL Div: 2360.9507\n",
            "Epoch[1/15], Step [360/469], Reconst Loss: 14848.9072, KL Div: 2354.3792\n",
            "Epoch[1/15], Step [370/469], Reconst Loss: 15139.1729, KL Div: 2577.2632\n",
            "Epoch[1/15], Step [380/469], Reconst Loss: 15026.2012, KL Div: 2439.7109\n",
            "Epoch[1/15], Step [390/469], Reconst Loss: 14270.4160, KL Div: 2403.9905\n",
            "Epoch[1/15], Step [400/469], Reconst Loss: 14245.8535, KL Div: 2493.8264\n",
            "Epoch[1/15], Step [410/469], Reconst Loss: 14044.5977, KL Div: 2458.7122\n",
            "Epoch[1/15], Step [420/469], Reconst Loss: 14129.4766, KL Div: 2525.8701\n",
            "Epoch[1/15], Step [430/469], Reconst Loss: 14295.2871, KL Div: 2680.6172\n",
            "Epoch[1/15], Step [440/469], Reconst Loss: 14282.2422, KL Div: 2548.4624\n",
            "Epoch[1/15], Step [450/469], Reconst Loss: 14345.9863, KL Div: 2608.5452\n",
            "Epoch[1/15], Step [460/469], Reconst Loss: 13288.2207, KL Div: 2582.2856\n",
            "Epoch[2/15], Step [10/469], Reconst Loss: 13710.8438, KL Div: 2628.6577\n",
            "Epoch[2/15], Step [20/469], Reconst Loss: 14173.1406, KL Div: 2506.5134\n",
            "Epoch[2/15], Step [30/469], Reconst Loss: 13631.5693, KL Div: 2741.9470\n",
            "Epoch[2/15], Step [40/469], Reconst Loss: 13545.7822, KL Div: 2792.2520\n",
            "Epoch[2/15], Step [50/469], Reconst Loss: 13616.2480, KL Div: 2653.6309\n",
            "Epoch[2/15], Step [60/469], Reconst Loss: 13608.1738, KL Div: 2713.3403\n",
            "Epoch[2/15], Step [70/469], Reconst Loss: 13363.7441, KL Div: 2673.4175\n",
            "Epoch[2/15], Step [80/469], Reconst Loss: 13276.8506, KL Div: 2651.2007\n",
            "Epoch[2/15], Step [90/469], Reconst Loss: 12445.4141, KL Div: 2722.3955\n",
            "Epoch[2/15], Step [100/469], Reconst Loss: 13124.5879, KL Div: 2721.5073\n",
            "Epoch[2/15], Step [110/469], Reconst Loss: 12798.6396, KL Div: 2757.5479\n",
            "Epoch[2/15], Step [120/469], Reconst Loss: 12776.9531, KL Div: 2687.2334\n",
            "Epoch[2/15], Step [130/469], Reconst Loss: 13420.5078, KL Div: 2739.4175\n",
            "Epoch[2/15], Step [140/469], Reconst Loss: 13237.9844, KL Div: 2767.1001\n",
            "Epoch[2/15], Step [150/469], Reconst Loss: 12573.5635, KL Div: 2873.0430\n",
            "Epoch[2/15], Step [160/469], Reconst Loss: 13309.4492, KL Div: 2869.4775\n",
            "Epoch[2/15], Step [170/469], Reconst Loss: 12580.1689, KL Div: 2811.9246\n",
            "Epoch[2/15], Step [180/469], Reconst Loss: 12797.1152, KL Div: 2825.1748\n",
            "Epoch[2/15], Step [190/469], Reconst Loss: 12913.2363, KL Div: 2714.1606\n",
            "Epoch[2/15], Step [200/469], Reconst Loss: 12891.4043, KL Div: 2819.1780\n",
            "Epoch[2/15], Step [210/469], Reconst Loss: 13140.1055, KL Div: 2779.3154\n",
            "Epoch[2/15], Step [220/469], Reconst Loss: 12541.5908, KL Div: 2810.4619\n",
            "Epoch[2/15], Step [230/469], Reconst Loss: 12645.8701, KL Div: 2815.3774\n",
            "Epoch[2/15], Step [240/469], Reconst Loss: 12704.8398, KL Div: 2907.6538\n",
            "Epoch[2/15], Step [250/469], Reconst Loss: 12572.6406, KL Div: 2833.7900\n",
            "Epoch[2/15], Step [260/469], Reconst Loss: 12844.2666, KL Div: 2771.3311\n",
            "Epoch[2/15], Step [270/469], Reconst Loss: 12446.7676, KL Div: 2906.3484\n",
            "Epoch[2/15], Step [280/469], Reconst Loss: 12569.0947, KL Div: 2789.5874\n",
            "Epoch[2/15], Step [290/469], Reconst Loss: 13349.0811, KL Div: 2965.2261\n",
            "Epoch[2/15], Step [300/469], Reconst Loss: 12645.3711, KL Div: 2948.4937\n",
            "Epoch[2/15], Step [310/469], Reconst Loss: 12317.1445, KL Div: 2905.3818\n",
            "Epoch[2/15], Step [320/469], Reconst Loss: 12000.7520, KL Div: 2816.7141\n",
            "Epoch[2/15], Step [330/469], Reconst Loss: 12958.9453, KL Div: 2922.9521\n",
            "Epoch[2/15], Step [340/469], Reconst Loss: 12493.2637, KL Div: 2958.6636\n",
            "Epoch[2/15], Step [350/469], Reconst Loss: 12219.8623, KL Div: 2933.2954\n",
            "Epoch[2/15], Step [360/469], Reconst Loss: 12324.2646, KL Div: 2994.8579\n",
            "Epoch[2/15], Step [370/469], Reconst Loss: 12252.9189, KL Div: 2915.2341\n",
            "Epoch[2/15], Step [380/469], Reconst Loss: 11786.8721, KL Div: 2869.7427\n",
            "Epoch[2/15], Step [390/469], Reconst Loss: 12191.4766, KL Div: 2818.5662\n",
            "Epoch[2/15], Step [400/469], Reconst Loss: 12234.4512, KL Div: 3002.0081\n",
            "Epoch[2/15], Step [410/469], Reconst Loss: 11726.3262, KL Div: 2849.4717\n",
            "Epoch[2/15], Step [420/469], Reconst Loss: 12110.8594, KL Div: 2961.6968\n",
            "Epoch[2/15], Step [430/469], Reconst Loss: 12301.9141, KL Div: 3028.5854\n",
            "Epoch[2/15], Step [440/469], Reconst Loss: 12051.3770, KL Div: 2885.1189\n",
            "Epoch[2/15], Step [450/469], Reconst Loss: 12052.3281, KL Div: 3016.0259\n",
            "Epoch[2/15], Step [460/469], Reconst Loss: 11585.5908, KL Div: 2949.7002\n",
            "Epoch[3/15], Step [10/469], Reconst Loss: 12290.5605, KL Div: 3083.5571\n",
            "Epoch[3/15], Step [20/469], Reconst Loss: 12068.7051, KL Div: 2937.4219\n",
            "Epoch[3/15], Step [30/469], Reconst Loss: 11283.3389, KL Div: 2962.4136\n",
            "Epoch[3/15], Step [40/469], Reconst Loss: 11697.9395, KL Div: 2996.8977\n",
            "Epoch[3/15], Step [50/469], Reconst Loss: 11978.8281, KL Div: 2961.5889\n",
            "Epoch[3/15], Step [60/469], Reconst Loss: 11620.5693, KL Div: 2967.1724\n",
            "Epoch[3/15], Step [70/469], Reconst Loss: 11608.8184, KL Div: 3057.4062\n",
            "Epoch[3/15], Step [80/469], Reconst Loss: 11920.0332, KL Div: 3014.8005\n",
            "Epoch[3/15], Step [90/469], Reconst Loss: 11958.6855, KL Div: 3007.8770\n",
            "Epoch[3/15], Step [100/469], Reconst Loss: 11952.5771, KL Div: 2920.5552\n",
            "Epoch[3/15], Step [110/469], Reconst Loss: 11463.8799, KL Div: 2962.1978\n",
            "Epoch[3/15], Step [120/469], Reconst Loss: 11908.7695, KL Div: 3040.0056\n",
            "Epoch[3/15], Step [130/469], Reconst Loss: 11503.6094, KL Div: 2941.5488\n",
            "Epoch[3/15], Step [140/469], Reconst Loss: 11628.2344, KL Div: 3017.6919\n",
            "Epoch[3/15], Step [150/469], Reconst Loss: 11657.3027, KL Div: 3125.2256\n",
            "Epoch[3/15], Step [160/469], Reconst Loss: 11573.2812, KL Div: 3015.5210\n",
            "Epoch[3/15], Step [170/469], Reconst Loss: 11665.4570, KL Div: 3106.7554\n",
            "Epoch[3/15], Step [180/469], Reconst Loss: 11796.4121, KL Div: 2968.5532\n",
            "Epoch[3/15], Step [190/469], Reconst Loss: 11967.2070, KL Div: 3146.5708\n",
            "Epoch[3/15], Step [200/469], Reconst Loss: 11811.2246, KL Div: 3000.6458\n",
            "Epoch[3/15], Step [210/469], Reconst Loss: 11904.5332, KL Div: 2981.8337\n",
            "Epoch[3/15], Step [220/469], Reconst Loss: 11698.2549, KL Div: 3202.6853\n",
            "Epoch[3/15], Step [230/469], Reconst Loss: 12041.0830, KL Div: 2928.7212\n",
            "Epoch[3/15], Step [240/469], Reconst Loss: 11297.9336, KL Div: 2995.5134\n",
            "Epoch[3/15], Step [250/469], Reconst Loss: 12561.0664, KL Div: 3232.5879\n",
            "Epoch[3/15], Step [260/469], Reconst Loss: 11356.5791, KL Div: 2944.8091\n",
            "Epoch[3/15], Step [270/469], Reconst Loss: 11190.9473, KL Div: 3158.7168\n",
            "Epoch[3/15], Step [280/469], Reconst Loss: 11247.3115, KL Div: 3120.0618\n",
            "Epoch[3/15], Step [290/469], Reconst Loss: 11539.1934, KL Div: 3030.6758\n",
            "Epoch[3/15], Step [300/469], Reconst Loss: 11626.7627, KL Div: 2975.0032\n",
            "Epoch[3/15], Step [310/469], Reconst Loss: 11427.2490, KL Div: 2974.0215\n",
            "Epoch[3/15], Step [320/469], Reconst Loss: 10954.7207, KL Div: 2990.8862\n",
            "Epoch[3/15], Step [330/469], Reconst Loss: 11463.0703, KL Div: 3117.2056\n",
            "Epoch[3/15], Step [340/469], Reconst Loss: 11546.9941, KL Div: 3019.7278\n",
            "Epoch[3/15], Step [350/469], Reconst Loss: 11555.2080, KL Div: 3043.2705\n",
            "Epoch[3/15], Step [360/469], Reconst Loss: 11559.0645, KL Div: 3093.2368\n",
            "Epoch[3/15], Step [370/469], Reconst Loss: 11513.5840, KL Div: 3083.5874\n",
            "Epoch[3/15], Step [380/469], Reconst Loss: 11187.2461, KL Div: 3047.7368\n",
            "Epoch[3/15], Step [390/469], Reconst Loss: 11718.5117, KL Div: 3022.1797\n",
            "Epoch[3/15], Step [400/469], Reconst Loss: 11288.3008, KL Div: 3114.7739\n",
            "Epoch[3/15], Step [410/469], Reconst Loss: 10845.3604, KL Div: 3023.9678\n",
            "Epoch[3/15], Step [420/469], Reconst Loss: 11476.9170, KL Div: 3061.6709\n",
            "Epoch[3/15], Step [430/469], Reconst Loss: 11802.5664, KL Div: 3132.9287\n",
            "Epoch[3/15], Step [440/469], Reconst Loss: 11530.7773, KL Div: 3166.1177\n",
            "Epoch[3/15], Step [450/469], Reconst Loss: 11785.3232, KL Div: 3110.2925\n",
            "Epoch[3/15], Step [460/469], Reconst Loss: 11599.4785, KL Div: 3038.9133\n",
            "Epoch[4/15], Step [10/469], Reconst Loss: 11047.9014, KL Div: 3016.6848\n",
            "Epoch[4/15], Step [20/469], Reconst Loss: 11000.8604, KL Div: 3102.2705\n",
            "Epoch[4/15], Step [30/469], Reconst Loss: 11587.5391, KL Div: 3177.4260\n",
            "Epoch[4/15], Step [40/469], Reconst Loss: 11143.2920, KL Div: 3052.1790\n",
            "Epoch[4/15], Step [50/469], Reconst Loss: 12021.1279, KL Div: 3305.5789\n",
            "Epoch[4/15], Step [60/469], Reconst Loss: 11488.3486, KL Div: 3033.6899\n",
            "Epoch[4/15], Step [70/469], Reconst Loss: 11202.0918, KL Div: 2961.0791\n",
            "Epoch[4/15], Step [80/469], Reconst Loss: 11129.8730, KL Div: 3164.9497\n",
            "Epoch[4/15], Step [90/469], Reconst Loss: 11373.0244, KL Div: 3034.8169\n",
            "Epoch[4/15], Step [100/469], Reconst Loss: 11403.8994, KL Div: 3003.7593\n",
            "Epoch[4/15], Step [110/469], Reconst Loss: 11782.8760, KL Div: 3149.4106\n",
            "Epoch[4/15], Step [120/469], Reconst Loss: 11399.2041, KL Div: 3152.8801\n",
            "Epoch[4/15], Step [130/469], Reconst Loss: 11132.2412, KL Div: 3074.9998\n",
            "Epoch[4/15], Step [140/469], Reconst Loss: 10903.3174, KL Div: 3159.3425\n",
            "Epoch[4/15], Step [150/469], Reconst Loss: 11525.8477, KL Div: 3058.2803\n",
            "Epoch[4/15], Step [160/469], Reconst Loss: 11342.9941, KL Div: 3050.0632\n",
            "Epoch[4/15], Step [170/469], Reconst Loss: 10575.3633, KL Div: 2991.6348\n",
            "Epoch[4/15], Step [180/469], Reconst Loss: 11574.7402, KL Div: 3218.7759\n",
            "Epoch[4/15], Step [190/469], Reconst Loss: 11003.5391, KL Div: 3112.7791\n",
            "Epoch[4/15], Step [200/469], Reconst Loss: 11280.2461, KL Div: 3119.9165\n",
            "Epoch[4/15], Step [210/469], Reconst Loss: 11284.0254, KL Div: 3135.6748\n",
            "Epoch[4/15], Step [220/469], Reconst Loss: 10872.4648, KL Div: 3054.7993\n",
            "Epoch[4/15], Step [230/469], Reconst Loss: 11433.5889, KL Div: 3215.1858\n",
            "Epoch[4/15], Step [240/469], Reconst Loss: 10798.1377, KL Div: 3167.8123\n",
            "Epoch[4/15], Step [250/469], Reconst Loss: 10987.1562, KL Div: 3116.3281\n",
            "Epoch[4/15], Step [260/469], Reconst Loss: 11081.1348, KL Div: 3063.7468\n",
            "Epoch[4/15], Step [270/469], Reconst Loss: 11135.0000, KL Div: 3165.5713\n",
            "Epoch[4/15], Step [280/469], Reconst Loss: 11621.1973, KL Div: 3200.1123\n",
            "Epoch[4/15], Step [290/469], Reconst Loss: 10778.2998, KL Div: 3117.4387\n",
            "Epoch[4/15], Step [300/469], Reconst Loss: 11476.5391, KL Div: 3209.3069\n",
            "Epoch[4/15], Step [310/469], Reconst Loss: 10735.9766, KL Div: 3005.6453\n",
            "Epoch[4/15], Step [320/469], Reconst Loss: 11392.1846, KL Div: 3169.9568\n",
            "Epoch[4/15], Step [330/469], Reconst Loss: 11241.0146, KL Div: 3187.9785\n",
            "Epoch[4/15], Step [340/469], Reconst Loss: 11274.2441, KL Div: 3167.5225\n",
            "Epoch[4/15], Step [350/469], Reconst Loss: 11445.0059, KL Div: 3123.2554\n",
            "Epoch[4/15], Step [360/469], Reconst Loss: 11165.2744, KL Div: 3134.9211\n",
            "Epoch[4/15], Step [370/469], Reconst Loss: 11350.5293, KL Div: 3081.5376\n",
            "Epoch[4/15], Step [380/469], Reconst Loss: 10997.2783, KL Div: 3286.5840\n",
            "Epoch[4/15], Step [390/469], Reconst Loss: 10579.7207, KL Div: 2948.6406\n",
            "Epoch[4/15], Step [400/469], Reconst Loss: 11089.1113, KL Div: 3109.0295\n",
            "Epoch[4/15], Step [410/469], Reconst Loss: 10678.9590, KL Div: 3149.4893\n",
            "Epoch[4/15], Step [420/469], Reconst Loss: 10634.9775, KL Div: 3084.6309\n",
            "Epoch[4/15], Step [430/469], Reconst Loss: 11062.2559, KL Div: 3251.5103\n",
            "Epoch[4/15], Step [440/469], Reconst Loss: 11200.1689, KL Div: 3122.0029\n",
            "Epoch[4/15], Step [450/469], Reconst Loss: 10531.4512, KL Div: 3127.5811\n",
            "Epoch[4/15], Step [460/469], Reconst Loss: 10911.3008, KL Div: 3109.5205\n",
            "Epoch[5/15], Step [10/469], Reconst Loss: 11244.2168, KL Div: 3288.0312\n",
            "Epoch[5/15], Step [20/469], Reconst Loss: 11119.9062, KL Div: 3195.4192\n",
            "Epoch[5/15], Step [30/469], Reconst Loss: 10408.0664, KL Div: 3172.9355\n",
            "Epoch[5/15], Step [40/469], Reconst Loss: 10825.2305, KL Div: 3044.2769\n",
            "Epoch[5/15], Step [50/469], Reconst Loss: 10893.9463, KL Div: 3148.7373\n",
            "Epoch[5/15], Step [60/469], Reconst Loss: 10990.1934, KL Div: 3109.1985\n",
            "Epoch[5/15], Step [70/469], Reconst Loss: 10640.1172, KL Div: 3136.9395\n",
            "Epoch[5/15], Step [80/469], Reconst Loss: 11042.8516, KL Div: 3124.8813\n",
            "Epoch[5/15], Step [90/469], Reconst Loss: 10791.7119, KL Div: 3163.5200\n",
            "Epoch[5/15], Step [100/469], Reconst Loss: 11057.7461, KL Div: 3249.9736\n",
            "Epoch[5/15], Step [110/469], Reconst Loss: 11021.4639, KL Div: 3124.3911\n",
            "Epoch[5/15], Step [120/469], Reconst Loss: 11071.6396, KL Div: 3271.6958\n",
            "Epoch[5/15], Step [130/469], Reconst Loss: 11128.0322, KL Div: 3101.7349\n",
            "Epoch[5/15], Step [140/469], Reconst Loss: 11020.5303, KL Div: 3112.5706\n",
            "Epoch[5/15], Step [150/469], Reconst Loss: 10257.2666, KL Div: 3185.3862\n",
            "Epoch[5/15], Step [160/469], Reconst Loss: 10888.2178, KL Div: 3076.0227\n",
            "Epoch[5/15], Step [170/469], Reconst Loss: 10878.9551, KL Div: 3156.3547\n",
            "Epoch[5/15], Step [180/469], Reconst Loss: 11109.3398, KL Div: 3150.5132\n",
            "Epoch[5/15], Step [190/469], Reconst Loss: 11085.9014, KL Div: 3084.2485\n",
            "Epoch[5/15], Step [200/469], Reconst Loss: 11020.8467, KL Div: 3174.9688\n",
            "Epoch[5/15], Step [210/469], Reconst Loss: 10670.4414, KL Div: 3047.4431\n",
            "Epoch[5/15], Step [220/469], Reconst Loss: 10885.6152, KL Div: 3293.6436\n",
            "Epoch[5/15], Step [230/469], Reconst Loss: 11290.6631, KL Div: 3178.0950\n",
            "Epoch[5/15], Step [240/469], Reconst Loss: 10776.6748, KL Div: 3174.4561\n",
            "Epoch[5/15], Step [250/469], Reconst Loss: 10796.1426, KL Div: 3150.2505\n",
            "Epoch[5/15], Step [260/469], Reconst Loss: 10638.4023, KL Div: 3151.9741\n",
            "Epoch[5/15], Step [270/469], Reconst Loss: 10743.8359, KL Div: 3128.3354\n",
            "Epoch[5/15], Step [280/469], Reconst Loss: 10960.9189, KL Div: 3087.1807\n",
            "Epoch[5/15], Step [290/469], Reconst Loss: 10599.7822, KL Div: 3082.0305\n",
            "Epoch[5/15], Step [300/469], Reconst Loss: 10675.6152, KL Div: 3140.8823\n",
            "Epoch[5/15], Step [310/469], Reconst Loss: 10752.1299, KL Div: 3198.4441\n",
            "Epoch[5/15], Step [320/469], Reconst Loss: 10973.1738, KL Div: 3202.0684\n",
            "Epoch[5/15], Step [330/469], Reconst Loss: 10626.3506, KL Div: 3130.1685\n",
            "Epoch[5/15], Step [340/469], Reconst Loss: 10371.2246, KL Div: 3125.6392\n",
            "Epoch[5/15], Step [350/469], Reconst Loss: 10962.6357, KL Div: 3092.5393\n",
            "Epoch[5/15], Step [360/469], Reconst Loss: 11392.6875, KL Div: 3214.3376\n",
            "Epoch[5/15], Step [370/469], Reconst Loss: 10877.5225, KL Div: 3190.5625\n",
            "Epoch[5/15], Step [380/469], Reconst Loss: 10600.5312, KL Div: 3136.5103\n",
            "Epoch[5/15], Step [390/469], Reconst Loss: 10993.5029, KL Div: 3213.0029\n",
            "Epoch[5/15], Step [400/469], Reconst Loss: 11111.9648, KL Div: 3047.1497\n",
            "Epoch[5/15], Step [410/469], Reconst Loss: 11045.5020, KL Div: 3252.6768\n",
            "Epoch[5/15], Step [420/469], Reconst Loss: 11346.5127, KL Div: 3115.9351\n",
            "Epoch[5/15], Step [430/469], Reconst Loss: 10553.9639, KL Div: 3238.3096\n",
            "Epoch[5/15], Step [440/469], Reconst Loss: 10596.0498, KL Div: 3168.1428\n",
            "Epoch[5/15], Step [450/469], Reconst Loss: 11150.2520, KL Div: 3205.4829\n",
            "Epoch[5/15], Step [460/469], Reconst Loss: 10966.8496, KL Div: 3134.7080\n",
            "Epoch[6/15], Step [10/469], Reconst Loss: 10894.0586, KL Div: 3210.7979\n",
            "Epoch[6/15], Step [20/469], Reconst Loss: 10814.0576, KL Div: 3078.4810\n",
            "Epoch[6/15], Step [30/469], Reconst Loss: 11045.2969, KL Div: 3140.8608\n",
            "Epoch[6/15], Step [40/469], Reconst Loss: 10459.6865, KL Div: 3135.4517\n",
            "Epoch[6/15], Step [50/469], Reconst Loss: 10693.5762, KL Div: 3316.4795\n",
            "Epoch[6/15], Step [60/469], Reconst Loss: 10917.3525, KL Div: 2981.9424\n",
            "Epoch[6/15], Step [70/469], Reconst Loss: 10644.8633, KL Div: 3189.2974\n",
            "Epoch[6/15], Step [80/469], Reconst Loss: 10743.4658, KL Div: 3154.6401\n",
            "Epoch[6/15], Step [90/469], Reconst Loss: 10944.9346, KL Div: 3109.7061\n",
            "Epoch[6/15], Step [100/469], Reconst Loss: 10960.0332, KL Div: 3149.6582\n",
            "Epoch[6/15], Step [110/469], Reconst Loss: 10384.5791, KL Div: 3159.2812\n",
            "Epoch[6/15], Step [120/469], Reconst Loss: 10340.3174, KL Div: 3219.1318\n",
            "Epoch[6/15], Step [130/469], Reconst Loss: 10492.9473, KL Div: 3136.3298\n",
            "Epoch[6/15], Step [140/469], Reconst Loss: 11163.6914, KL Div: 3076.8545\n",
            "Epoch[6/15], Step [150/469], Reconst Loss: 10694.3945, KL Div: 3206.9595\n",
            "Epoch[6/15], Step [160/469], Reconst Loss: 10735.1504, KL Div: 3230.8940\n",
            "Epoch[6/15], Step [170/469], Reconst Loss: 11002.5039, KL Div: 3052.6997\n",
            "Epoch[6/15], Step [180/469], Reconst Loss: 10364.1963, KL Div: 3110.4612\n",
            "Epoch[6/15], Step [190/469], Reconst Loss: 10147.8164, KL Div: 3143.9673\n",
            "Epoch[6/15], Step [200/469], Reconst Loss: 10722.3887, KL Div: 3256.0361\n",
            "Epoch[6/15], Step [210/469], Reconst Loss: 10247.9336, KL Div: 3088.2043\n",
            "Epoch[6/15], Step [220/469], Reconst Loss: 11182.4355, KL Div: 3144.5413\n",
            "Epoch[6/15], Step [230/469], Reconst Loss: 10602.7178, KL Div: 3250.3892\n",
            "Epoch[6/15], Step [240/469], Reconst Loss: 10706.8711, KL Div: 3088.1438\n",
            "Epoch[6/15], Step [250/469], Reconst Loss: 10570.9854, KL Div: 3215.3901\n",
            "Epoch[6/15], Step [260/469], Reconst Loss: 11224.6660, KL Div: 3193.0532\n",
            "Epoch[6/15], Step [270/469], Reconst Loss: 10984.7812, KL Div: 3220.8689\n",
            "Epoch[6/15], Step [280/469], Reconst Loss: 10804.4883, KL Div: 3226.7151\n",
            "Epoch[6/15], Step [290/469], Reconst Loss: 10368.2188, KL Div: 3189.1489\n",
            "Epoch[6/15], Step [300/469], Reconst Loss: 10775.4922, KL Div: 3173.8162\n",
            "Epoch[6/15], Step [310/469], Reconst Loss: 10394.4258, KL Div: 3127.5444\n",
            "Epoch[6/15], Step [320/469], Reconst Loss: 10783.0928, KL Div: 3295.6858\n",
            "Epoch[6/15], Step [330/469], Reconst Loss: 10889.8574, KL Div: 3096.2158\n",
            "Epoch[6/15], Step [340/469], Reconst Loss: 10482.9941, KL Div: 3178.0181\n",
            "Epoch[6/15], Step [350/469], Reconst Loss: 10752.7344, KL Div: 3180.4075\n",
            "Epoch[6/15], Step [360/469], Reconst Loss: 10770.9893, KL Div: 3190.0796\n",
            "Epoch[6/15], Step [370/469], Reconst Loss: 10287.4219, KL Div: 3089.2878\n",
            "Epoch[6/15], Step [380/469], Reconst Loss: 10525.0957, KL Div: 3181.9065\n",
            "Epoch[6/15], Step [390/469], Reconst Loss: 10739.9258, KL Div: 3230.7969\n",
            "Epoch[6/15], Step [400/469], Reconst Loss: 11037.0342, KL Div: 3113.7371\n",
            "Epoch[6/15], Step [410/469], Reconst Loss: 10370.7061, KL Div: 3171.6111\n",
            "Epoch[6/15], Step [420/469], Reconst Loss: 10753.0947, KL Div: 3163.7251\n",
            "Epoch[6/15], Step [430/469], Reconst Loss: 10801.1387, KL Div: 3238.5444\n",
            "Epoch[6/15], Step [440/469], Reconst Loss: 11049.9297, KL Div: 3123.9292\n",
            "Epoch[6/15], Step [450/469], Reconst Loss: 11029.6348, KL Div: 3253.5830\n",
            "Epoch[6/15], Step [460/469], Reconst Loss: 10690.6807, KL Div: 3227.8625\n",
            "Epoch[7/15], Step [10/469], Reconst Loss: 10838.2461, KL Div: 3334.6309\n",
            "Epoch[7/15], Step [20/469], Reconst Loss: 10536.5420, KL Div: 3187.3662\n",
            "Epoch[7/15], Step [30/469], Reconst Loss: 10493.0645, KL Div: 3096.1426\n",
            "Epoch[7/15], Step [40/469], Reconst Loss: 10092.2158, KL Div: 3268.0239\n",
            "Epoch[7/15], Step [50/469], Reconst Loss: 10625.5391, KL Div: 3217.7385\n",
            "Epoch[7/15], Step [60/469], Reconst Loss: 10490.1260, KL Div: 3235.2119\n",
            "Epoch[7/15], Step [70/469], Reconst Loss: 10752.4180, KL Div: 3159.9458\n",
            "Epoch[7/15], Step [80/469], Reconst Loss: 10336.7812, KL Div: 3173.4409\n",
            "Epoch[7/15], Step [90/469], Reconst Loss: 10627.6562, KL Div: 3132.9316\n",
            "Epoch[7/15], Step [100/469], Reconst Loss: 10198.0312, KL Div: 3190.5256\n",
            "Epoch[7/15], Step [110/469], Reconst Loss: 10538.6484, KL Div: 3238.3140\n",
            "Epoch[7/15], Step [120/469], Reconst Loss: 10226.2109, KL Div: 3124.0862\n",
            "Epoch[7/15], Step [130/469], Reconst Loss: 10945.3691, KL Div: 3279.9097\n",
            "Epoch[7/15], Step [140/469], Reconst Loss: 10841.9834, KL Div: 3177.7073\n",
            "Epoch[7/15], Step [150/469], Reconst Loss: 10652.0029, KL Div: 3160.0464\n",
            "Epoch[7/15], Step [160/469], Reconst Loss: 10758.8516, KL Div: 3262.2280\n",
            "Epoch[7/15], Step [170/469], Reconst Loss: 10899.2168, KL Div: 3117.4109\n",
            "Epoch[7/15], Step [180/469], Reconst Loss: 10347.8311, KL Div: 3223.6289\n",
            "Epoch[7/15], Step [190/469], Reconst Loss: 11056.7471, KL Div: 3339.8052\n",
            "Epoch[7/15], Step [200/469], Reconst Loss: 10189.1182, KL Div: 3145.0322\n",
            "Epoch[7/15], Step [210/469], Reconst Loss: 10743.4727, KL Div: 3238.1292\n",
            "Epoch[7/15], Step [220/469], Reconst Loss: 10369.0547, KL Div: 3047.2656\n",
            "Epoch[7/15], Step [230/469], Reconst Loss: 10560.1201, KL Div: 3254.6331\n",
            "Epoch[7/15], Step [240/469], Reconst Loss: 10547.9434, KL Div: 3096.4963\n",
            "Epoch[7/15], Step [250/469], Reconst Loss: 10365.7598, KL Div: 3110.6992\n",
            "Epoch[7/15], Step [260/469], Reconst Loss: 10837.7773, KL Div: 3254.3926\n",
            "Epoch[7/15], Step [270/469], Reconst Loss: 10678.5664, KL Div: 3128.4150\n",
            "Epoch[7/15], Step [280/469], Reconst Loss: 10348.4180, KL Div: 3210.1047\n",
            "Epoch[7/15], Step [290/469], Reconst Loss: 10693.6191, KL Div: 3262.6970\n",
            "Epoch[7/15], Step [300/469], Reconst Loss: 10586.6699, KL Div: 3162.7720\n",
            "Epoch[7/15], Step [310/469], Reconst Loss: 11064.2109, KL Div: 3229.8755\n",
            "Epoch[7/15], Step [320/469], Reconst Loss: 11022.8438, KL Div: 3233.0173\n",
            "Epoch[7/15], Step [330/469], Reconst Loss: 10979.1328, KL Div: 3194.4653\n",
            "Epoch[7/15], Step [340/469], Reconst Loss: 10246.6553, KL Div: 3179.3818\n",
            "Epoch[7/15], Step [350/469], Reconst Loss: 10967.0752, KL Div: 3155.3110\n",
            "Epoch[7/15], Step [360/469], Reconst Loss: 10897.6582, KL Div: 3276.6306\n",
            "Epoch[7/15], Step [370/469], Reconst Loss: 10401.6592, KL Div: 3078.5698\n",
            "Epoch[7/15], Step [380/469], Reconst Loss: 10634.6113, KL Div: 3205.5156\n",
            "Epoch[7/15], Step [390/469], Reconst Loss: 10387.4482, KL Div: 3128.0281\n",
            "Epoch[7/15], Step [400/469], Reconst Loss: 10482.2979, KL Div: 3146.8103\n",
            "Epoch[7/15], Step [410/469], Reconst Loss: 10915.4941, KL Div: 3220.3630\n",
            "Epoch[7/15], Step [420/469], Reconst Loss: 10647.3564, KL Div: 3106.8867\n",
            "Epoch[7/15], Step [430/469], Reconst Loss: 10448.1299, KL Div: 3193.4861\n",
            "Epoch[7/15], Step [440/469], Reconst Loss: 10830.7441, KL Div: 3111.9385\n",
            "Epoch[7/15], Step [450/469], Reconst Loss: 10436.3408, KL Div: 3206.4495\n",
            "Epoch[7/15], Step [460/469], Reconst Loss: 10251.2412, KL Div: 3065.3208\n",
            "Epoch[8/15], Step [10/469], Reconst Loss: 10347.0576, KL Div: 3120.4875\n",
            "Epoch[8/15], Step [20/469], Reconst Loss: 10456.5938, KL Div: 3211.1260\n",
            "Epoch[8/15], Step [30/469], Reconst Loss: 10150.1816, KL Div: 3177.8901\n",
            "Epoch[8/15], Step [40/469], Reconst Loss: 10778.4150, KL Div: 3258.5789\n",
            "Epoch[8/15], Step [50/469], Reconst Loss: 10472.6729, KL Div: 3151.1331\n",
            "Epoch[8/15], Step [60/469], Reconst Loss: 10720.3848, KL Div: 3235.6221\n",
            "Epoch[8/15], Step [70/469], Reconst Loss: 10404.2178, KL Div: 3154.8528\n",
            "Epoch[8/15], Step [80/469], Reconst Loss: 10047.0039, KL Div: 3140.0884\n",
            "Epoch[8/15], Step [90/469], Reconst Loss: 10623.8975, KL Div: 3253.5574\n",
            "Epoch[8/15], Step [100/469], Reconst Loss: 10396.8711, KL Div: 3217.7407\n",
            "Epoch[8/15], Step [110/469], Reconst Loss: 10933.0918, KL Div: 3172.4097\n",
            "Epoch[8/15], Step [120/469], Reconst Loss: 10666.6914, KL Div: 3142.8804\n",
            "Epoch[8/15], Step [130/469], Reconst Loss: 10863.9873, KL Div: 3243.2363\n",
            "Epoch[8/15], Step [140/469], Reconst Loss: 10589.8193, KL Div: 3092.0449\n",
            "Epoch[8/15], Step [150/469], Reconst Loss: 10100.3027, KL Div: 3130.4897\n",
            "Epoch[8/15], Step [160/469], Reconst Loss: 10503.2520, KL Div: 3175.3384\n",
            "Epoch[8/15], Step [170/469], Reconst Loss: 10617.6797, KL Div: 3312.9387\n",
            "Epoch[8/15], Step [180/469], Reconst Loss: 10745.0586, KL Div: 3231.7070\n",
            "Epoch[8/15], Step [190/469], Reconst Loss: 10444.4004, KL Div: 3195.6760\n",
            "Epoch[8/15], Step [200/469], Reconst Loss: 10348.6719, KL Div: 3131.3281\n",
            "Epoch[8/15], Step [210/469], Reconst Loss: 10653.4268, KL Div: 3300.3335\n",
            "Epoch[8/15], Step [220/469], Reconst Loss: 10672.5781, KL Div: 3205.1006\n",
            "Epoch[8/15], Step [230/469], Reconst Loss: 10830.1719, KL Div: 3150.1675\n",
            "Epoch[8/15], Step [240/469], Reconst Loss: 10284.0977, KL Div: 3142.5708\n",
            "Epoch[8/15], Step [250/469], Reconst Loss: 10556.9336, KL Div: 3276.4829\n",
            "Epoch[8/15], Step [260/469], Reconst Loss: 9910.9219, KL Div: 3080.6799\n",
            "Epoch[8/15], Step [270/469], Reconst Loss: 10311.4512, KL Div: 3151.3701\n",
            "Epoch[8/15], Step [280/469], Reconst Loss: 10775.4580, KL Div: 3199.1245\n",
            "Epoch[8/15], Step [290/469], Reconst Loss: 10049.3242, KL Div: 3186.2148\n",
            "Epoch[8/15], Step [300/469], Reconst Loss: 10468.1973, KL Div: 3212.0007\n",
            "Epoch[8/15], Step [310/469], Reconst Loss: 10370.4600, KL Div: 3197.5498\n",
            "Epoch[8/15], Step [320/469], Reconst Loss: 10323.4502, KL Div: 3023.3911\n",
            "Epoch[8/15], Step [330/469], Reconst Loss: 9793.1396, KL Div: 3109.6328\n",
            "Epoch[8/15], Step [340/469], Reconst Loss: 10380.9902, KL Div: 3085.5928\n",
            "Epoch[8/15], Step [350/469], Reconst Loss: 10683.9365, KL Div: 3302.4351\n",
            "Epoch[8/15], Step [360/469], Reconst Loss: 10720.0371, KL Div: 3128.7529\n",
            "Epoch[8/15], Step [370/469], Reconst Loss: 10787.5234, KL Div: 3258.3018\n",
            "Epoch[8/15], Step [380/469], Reconst Loss: 10869.2666, KL Div: 3274.9231\n",
            "Epoch[8/15], Step [390/469], Reconst Loss: 10454.4082, KL Div: 3023.6150\n",
            "Epoch[8/15], Step [400/469], Reconst Loss: 10205.5615, KL Div: 3296.8457\n",
            "Epoch[8/15], Step [410/469], Reconst Loss: 10512.1934, KL Div: 3142.5361\n",
            "Epoch[8/15], Step [420/469], Reconst Loss: 10339.8496, KL Div: 3194.7209\n",
            "Epoch[8/15], Step [430/469], Reconst Loss: 10512.9326, KL Div: 3211.5366\n",
            "Epoch[8/15], Step [440/469], Reconst Loss: 10524.6182, KL Div: 3241.4756\n",
            "Epoch[8/15], Step [450/469], Reconst Loss: 10434.8848, KL Div: 3144.9097\n",
            "Epoch[8/15], Step [460/469], Reconst Loss: 10427.7373, KL Div: 3215.2883\n",
            "Epoch[9/15], Step [10/469], Reconst Loss: 10536.2998, KL Div: 3288.9756\n",
            "Epoch[9/15], Step [20/469], Reconst Loss: 9971.6768, KL Div: 3105.5747\n",
            "Epoch[9/15], Step [30/469], Reconst Loss: 10640.1670, KL Div: 3146.3062\n",
            "Epoch[9/15], Step [40/469], Reconst Loss: 10240.0557, KL Div: 3187.3408\n",
            "Epoch[9/15], Step [50/469], Reconst Loss: 10294.2930, KL Div: 3239.3467\n",
            "Epoch[9/15], Step [60/469], Reconst Loss: 10756.3311, KL Div: 3203.2085\n",
            "Epoch[9/15], Step [70/469], Reconst Loss: 10137.9023, KL Div: 3103.2224\n",
            "Epoch[9/15], Step [80/469], Reconst Loss: 10516.9434, KL Div: 3273.0264\n",
            "Epoch[9/15], Step [90/469], Reconst Loss: 10491.4307, KL Div: 3130.9949\n",
            "Epoch[9/15], Step [100/469], Reconst Loss: 10195.6523, KL Div: 3230.8896\n",
            "Epoch[9/15], Step [110/469], Reconst Loss: 10311.0801, KL Div: 3235.9180\n",
            "Epoch[9/15], Step [120/469], Reconst Loss: 10236.0479, KL Div: 3213.3857\n",
            "Epoch[9/15], Step [130/469], Reconst Loss: 10438.0312, KL Div: 3082.6548\n",
            "Epoch[9/15], Step [140/469], Reconst Loss: 10827.2051, KL Div: 3291.5195\n",
            "Epoch[9/15], Step [150/469], Reconst Loss: 10123.9863, KL Div: 3144.8435\n",
            "Epoch[9/15], Step [160/469], Reconst Loss: 10601.9082, KL Div: 3219.4812\n",
            "Epoch[9/15], Step [170/469], Reconst Loss: 10347.2568, KL Div: 3038.2275\n",
            "Epoch[9/15], Step [180/469], Reconst Loss: 10547.5244, KL Div: 3189.2944\n",
            "Epoch[9/15], Step [190/469], Reconst Loss: 10257.0410, KL Div: 3253.2610\n",
            "Epoch[9/15], Step [200/469], Reconst Loss: 10518.8926, KL Div: 3239.6890\n",
            "Epoch[9/15], Step [210/469], Reconst Loss: 9843.2764, KL Div: 3103.4653\n",
            "Epoch[9/15], Step [220/469], Reconst Loss: 10250.0664, KL Div: 3283.9338\n",
            "Epoch[9/15], Step [230/469], Reconst Loss: 9982.1133, KL Div: 3120.0381\n",
            "Epoch[9/15], Step [240/469], Reconst Loss: 10349.2852, KL Div: 3262.5708\n",
            "Epoch[9/15], Step [250/469], Reconst Loss: 10436.0205, KL Div: 3350.6401\n",
            "Epoch[9/15], Step [260/469], Reconst Loss: 10528.6875, KL Div: 3147.6182\n",
            "Epoch[9/15], Step [270/469], Reconst Loss: 10029.9414, KL Div: 3294.4023\n",
            "Epoch[9/15], Step [280/469], Reconst Loss: 10986.9629, KL Div: 3150.0996\n",
            "Epoch[9/15], Step [290/469], Reconst Loss: 10795.1016, KL Div: 3315.3552\n",
            "Epoch[9/15], Step [300/469], Reconst Loss: 10438.0195, KL Div: 3259.1987\n",
            "Epoch[9/15], Step [310/469], Reconst Loss: 10796.2783, KL Div: 3257.8384\n",
            "Epoch[9/15], Step [320/469], Reconst Loss: 10199.7119, KL Div: 3152.9316\n",
            "Epoch[9/15], Step [330/469], Reconst Loss: 9864.8271, KL Div: 3212.0347\n",
            "Epoch[9/15], Step [340/469], Reconst Loss: 10312.0186, KL Div: 3226.3730\n",
            "Epoch[9/15], Step [350/469], Reconst Loss: 10722.2109, KL Div: 3266.2170\n",
            "Epoch[9/15], Step [360/469], Reconst Loss: 10542.7959, KL Div: 3144.0542\n",
            "Epoch[9/15], Step [370/469], Reconst Loss: 10744.0195, KL Div: 3215.9480\n",
            "Epoch[9/15], Step [380/469], Reconst Loss: 10745.7207, KL Div: 3248.4451\n",
            "Epoch[9/15], Step [390/469], Reconst Loss: 10802.2734, KL Div: 3296.3425\n",
            "Epoch[9/15], Step [400/469], Reconst Loss: 10556.4316, KL Div: 3200.6064\n",
            "Epoch[9/15], Step [410/469], Reconst Loss: 10580.3496, KL Div: 3246.9985\n",
            "Epoch[9/15], Step [420/469], Reconst Loss: 10294.6680, KL Div: 3060.1792\n",
            "Epoch[9/15], Step [430/469], Reconst Loss: 9819.4707, KL Div: 3226.3247\n",
            "Epoch[9/15], Step [440/469], Reconst Loss: 10020.2500, KL Div: 3146.6797\n",
            "Epoch[9/15], Step [450/469], Reconst Loss: 10620.5410, KL Div: 3189.4463\n",
            "Epoch[9/15], Step [460/469], Reconst Loss: 10454.0049, KL Div: 3224.2021\n",
            "Epoch[10/15], Step [10/469], Reconst Loss: 9703.3916, KL Div: 3096.6514\n",
            "Epoch[10/15], Step [20/469], Reconst Loss: 10258.7979, KL Div: 3221.2849\n",
            "Epoch[10/15], Step [30/469], Reconst Loss: 10271.0674, KL Div: 3114.4077\n",
            "Epoch[10/15], Step [40/469], Reconst Loss: 10526.6211, KL Div: 3227.8093\n",
            "Epoch[10/15], Step [50/469], Reconst Loss: 10549.1768, KL Div: 3291.6340\n",
            "Epoch[10/15], Step [60/469], Reconst Loss: 10811.8311, KL Div: 3118.4946\n",
            "Epoch[10/15], Step [70/469], Reconst Loss: 10265.8125, KL Div: 3154.2324\n",
            "Epoch[10/15], Step [80/469], Reconst Loss: 10757.3867, KL Div: 3272.1897\n",
            "Epoch[10/15], Step [90/469], Reconst Loss: 10333.8164, KL Div: 3164.0632\n",
            "Epoch[10/15], Step [100/469], Reconst Loss: 10551.1689, KL Div: 3248.5598\n",
            "Epoch[10/15], Step [110/469], Reconst Loss: 10549.6445, KL Div: 3332.7146\n",
            "Epoch[10/15], Step [120/469], Reconst Loss: 10642.1865, KL Div: 3176.0203\n",
            "Epoch[10/15], Step [130/469], Reconst Loss: 10429.9492, KL Div: 3293.8594\n",
            "Epoch[10/15], Step [140/469], Reconst Loss: 10067.3955, KL Div: 3167.5513\n",
            "Epoch[10/15], Step [150/469], Reconst Loss: 10782.1270, KL Div: 3150.1365\n",
            "Epoch[10/15], Step [160/469], Reconst Loss: 10115.5283, KL Div: 3169.0527\n",
            "Epoch[10/15], Step [170/469], Reconst Loss: 10508.1201, KL Div: 3208.4658\n",
            "Epoch[10/15], Step [180/469], Reconst Loss: 9802.8223, KL Div: 3211.5764\n",
            "Epoch[10/15], Step [190/469], Reconst Loss: 10653.0420, KL Div: 3239.9492\n",
            "Epoch[10/15], Step [200/469], Reconst Loss: 10415.0801, KL Div: 3281.6130\n",
            "Epoch[10/15], Step [210/469], Reconst Loss: 10158.0225, KL Div: 3168.2812\n",
            "Epoch[10/15], Step [220/469], Reconst Loss: 10461.7764, KL Div: 3223.9385\n",
            "Epoch[10/15], Step [230/469], Reconst Loss: 10401.8945, KL Div: 3270.4365\n",
            "Epoch[10/15], Step [240/469], Reconst Loss: 10309.3604, KL Div: 3234.8760\n",
            "Epoch[10/15], Step [250/469], Reconst Loss: 10540.5625, KL Div: 3229.9404\n",
            "Epoch[10/15], Step [260/469], Reconst Loss: 10250.6650, KL Div: 3259.0718\n",
            "Epoch[10/15], Step [270/469], Reconst Loss: 10269.9902, KL Div: 3264.9482\n",
            "Epoch[10/15], Step [280/469], Reconst Loss: 10410.7100, KL Div: 3273.3965\n",
            "Epoch[10/15], Step [290/469], Reconst Loss: 10047.9609, KL Div: 3115.1460\n",
            "Epoch[10/15], Step [300/469], Reconst Loss: 10596.4365, KL Div: 3161.9353\n",
            "Epoch[10/15], Step [310/469], Reconst Loss: 10258.1826, KL Div: 3217.0828\n",
            "Epoch[10/15], Step [320/469], Reconst Loss: 10684.1064, KL Div: 3216.7974\n",
            "Epoch[10/15], Step [330/469], Reconst Loss: 10304.9736, KL Div: 3170.4817\n",
            "Epoch[10/15], Step [340/469], Reconst Loss: 10028.7188, KL Div: 3080.5977\n",
            "Epoch[10/15], Step [350/469], Reconst Loss: 10299.9941, KL Div: 3159.9902\n",
            "Epoch[10/15], Step [360/469], Reconst Loss: 10544.8926, KL Div: 3220.8276\n",
            "Epoch[10/15], Step [370/469], Reconst Loss: 10446.3457, KL Div: 3193.2913\n",
            "Epoch[10/15], Step [380/469], Reconst Loss: 10613.1152, KL Div: 3364.0247\n",
            "Epoch[10/15], Step [390/469], Reconst Loss: 10452.8594, KL Div: 3102.2937\n",
            "Epoch[10/15], Step [400/469], Reconst Loss: 10573.0039, KL Div: 3244.2324\n",
            "Epoch[10/15], Step [410/469], Reconst Loss: 10311.7109, KL Div: 3210.4792\n",
            "Epoch[10/15], Step [420/469], Reconst Loss: 10219.1152, KL Div: 3232.8730\n",
            "Epoch[10/15], Step [430/469], Reconst Loss: 10236.3496, KL Div: 3197.6772\n",
            "Epoch[10/15], Step [440/469], Reconst Loss: 10219.0527, KL Div: 3083.1504\n",
            "Epoch[10/15], Step [450/469], Reconst Loss: 10336.8682, KL Div: 3253.5415\n",
            "Epoch[10/15], Step [460/469], Reconst Loss: 10354.8535, KL Div: 3148.7764\n",
            "Epoch[11/15], Step [10/469], Reconst Loss: 10204.8730, KL Div: 3166.0872\n",
            "Epoch[11/15], Step [20/469], Reconst Loss: 10145.9570, KL Div: 3197.0498\n",
            "Epoch[11/15], Step [30/469], Reconst Loss: 10322.2217, KL Div: 3269.2949\n",
            "Epoch[11/15], Step [40/469], Reconst Loss: 10386.5381, KL Div: 3270.2632\n",
            "Epoch[11/15], Step [50/469], Reconst Loss: 10313.7529, KL Div: 3267.5906\n",
            "Epoch[11/15], Step [60/469], Reconst Loss: 10291.8486, KL Div: 3269.2139\n",
            "Epoch[11/15], Step [70/469], Reconst Loss: 10408.5840, KL Div: 3227.1458\n",
            "Epoch[11/15], Step [80/469], Reconst Loss: 10508.5566, KL Div: 3145.2124\n",
            "Epoch[11/15], Step [90/469], Reconst Loss: 10258.8555, KL Div: 3330.4924\n",
            "Epoch[11/15], Step [100/469], Reconst Loss: 9911.6895, KL Div: 3250.9688\n",
            "Epoch[11/15], Step [110/469], Reconst Loss: 10414.1982, KL Div: 3178.1243\n",
            "Epoch[11/15], Step [120/469], Reconst Loss: 10167.3535, KL Div: 3267.0938\n",
            "Epoch[11/15], Step [130/469], Reconst Loss: 9999.6475, KL Div: 3031.4150\n",
            "Epoch[11/15], Step [140/469], Reconst Loss: 10581.9414, KL Div: 3230.8938\n",
            "Epoch[11/15], Step [150/469], Reconst Loss: 10844.0742, KL Div: 3246.5752\n",
            "Epoch[11/15], Step [160/469], Reconst Loss: 10433.7354, KL Div: 3158.6475\n",
            "Epoch[11/15], Step [170/469], Reconst Loss: 10343.4639, KL Div: 3283.1836\n",
            "Epoch[11/15], Step [180/469], Reconst Loss: 10044.4111, KL Div: 3136.9082\n",
            "Epoch[11/15], Step [190/469], Reconst Loss: 10337.0186, KL Div: 3180.3271\n",
            "Epoch[11/15], Step [200/469], Reconst Loss: 10537.9219, KL Div: 3240.0220\n",
            "Epoch[11/15], Step [210/469], Reconst Loss: 10167.2520, KL Div: 3191.4644\n",
            "Epoch[11/15], Step [220/469], Reconst Loss: 10070.6484, KL Div: 3204.2383\n",
            "Epoch[11/15], Step [230/469], Reconst Loss: 10590.9873, KL Div: 3319.3020\n",
            "Epoch[11/15], Step [240/469], Reconst Loss: 10622.0947, KL Div: 3092.5095\n",
            "Epoch[11/15], Step [250/469], Reconst Loss: 10595.4766, KL Div: 3257.1123\n",
            "Epoch[11/15], Step [260/469], Reconst Loss: 10391.6523, KL Div: 3216.4453\n",
            "Epoch[11/15], Step [270/469], Reconst Loss: 10021.0049, KL Div: 3048.8206\n",
            "Epoch[11/15], Step [280/469], Reconst Loss: 10028.8027, KL Div: 3264.5449\n",
            "Epoch[11/15], Step [290/469], Reconst Loss: 10632.6699, KL Div: 3367.6145\n",
            "Epoch[11/15], Step [300/469], Reconst Loss: 10793.8506, KL Div: 3146.3518\n",
            "Epoch[11/15], Step [310/469], Reconst Loss: 10421.1074, KL Div: 3274.8643\n",
            "Epoch[11/15], Step [320/469], Reconst Loss: 10596.6758, KL Div: 3225.4878\n",
            "Epoch[11/15], Step [330/469], Reconst Loss: 10259.7725, KL Div: 3304.5122\n",
            "Epoch[11/15], Step [340/469], Reconst Loss: 10300.8076, KL Div: 3185.0820\n",
            "Epoch[11/15], Step [350/469], Reconst Loss: 10919.5342, KL Div: 3235.4023\n",
            "Epoch[11/15], Step [360/469], Reconst Loss: 10388.1904, KL Div: 3199.7349\n",
            "Epoch[11/15], Step [370/469], Reconst Loss: 10153.3291, KL Div: 3235.8440\n",
            "Epoch[11/15], Step [380/469], Reconst Loss: 10182.0723, KL Div: 3192.1182\n",
            "Epoch[11/15], Step [390/469], Reconst Loss: 10298.0654, KL Div: 3228.5874\n",
            "Epoch[11/15], Step [400/469], Reconst Loss: 10142.1250, KL Div: 3152.8330\n",
            "Epoch[11/15], Step [410/469], Reconst Loss: 10578.3213, KL Div: 3356.0247\n",
            "Epoch[11/15], Step [420/469], Reconst Loss: 10085.6895, KL Div: 3191.2915\n",
            "Epoch[11/15], Step [430/469], Reconst Loss: 9857.1836, KL Div: 3124.2173\n",
            "Epoch[11/15], Step [440/469], Reconst Loss: 10077.1055, KL Div: 3238.3503\n",
            "Epoch[11/15], Step [450/469], Reconst Loss: 10499.3008, KL Div: 3321.2246\n",
            "Epoch[11/15], Step [460/469], Reconst Loss: 10111.1240, KL Div: 3063.8447\n",
            "Epoch[12/15], Step [10/469], Reconst Loss: 9907.8408, KL Div: 3151.5852\n",
            "Epoch[12/15], Step [20/469], Reconst Loss: 10525.2168, KL Div: 3268.9575\n",
            "Epoch[12/15], Step [30/469], Reconst Loss: 10339.7471, KL Div: 3173.2065\n",
            "Epoch[12/15], Step [40/469], Reconst Loss: 10362.4150, KL Div: 3225.3013\n",
            "Epoch[12/15], Step [50/469], Reconst Loss: 10612.4570, KL Div: 3351.1133\n",
            "Epoch[12/15], Step [60/469], Reconst Loss: 10280.8818, KL Div: 3137.4688\n",
            "Epoch[12/15], Step [70/469], Reconst Loss: 10182.1602, KL Div: 3244.4189\n",
            "Epoch[12/15], Step [80/469], Reconst Loss: 10844.4658, KL Div: 3281.5835\n",
            "Epoch[12/15], Step [90/469], Reconst Loss: 10280.9209, KL Div: 3207.4441\n",
            "Epoch[12/15], Step [100/469], Reconst Loss: 10582.0557, KL Div: 3264.2295\n",
            "Epoch[12/15], Step [110/469], Reconst Loss: 10587.5850, KL Div: 3215.3315\n",
            "Epoch[12/15], Step [120/469], Reconst Loss: 10425.1123, KL Div: 3264.1870\n",
            "Epoch[12/15], Step [130/469], Reconst Loss: 10027.7539, KL Div: 3223.2773\n",
            "Epoch[12/15], Step [140/469], Reconst Loss: 10312.3770, KL Div: 3295.7639\n",
            "Epoch[12/15], Step [150/469], Reconst Loss: 10358.0605, KL Div: 3180.1284\n",
            "Epoch[12/15], Step [160/469], Reconst Loss: 10202.7988, KL Div: 3045.4106\n",
            "Epoch[12/15], Step [170/469], Reconst Loss: 10477.7637, KL Div: 3342.6719\n",
            "Epoch[12/15], Step [180/469], Reconst Loss: 10192.6035, KL Div: 3320.1035\n",
            "Epoch[12/15], Step [190/469], Reconst Loss: 10516.0391, KL Div: 3168.3276\n",
            "Epoch[12/15], Step [200/469], Reconst Loss: 10272.8955, KL Div: 3211.9341\n",
            "Epoch[12/15], Step [210/469], Reconst Loss: 10540.1035, KL Div: 3309.1069\n",
            "Epoch[12/15], Step [220/469], Reconst Loss: 10065.4893, KL Div: 3191.1575\n",
            "Epoch[12/15], Step [230/469], Reconst Loss: 10374.8574, KL Div: 3259.4668\n",
            "Epoch[12/15], Step [240/469], Reconst Loss: 10331.7744, KL Div: 3316.1218\n",
            "Epoch[12/15], Step [250/469], Reconst Loss: 10547.4922, KL Div: 3205.0117\n",
            "Epoch[12/15], Step [260/469], Reconst Loss: 10065.7207, KL Div: 3087.1138\n",
            "Epoch[12/15], Step [270/469], Reconst Loss: 10125.6680, KL Div: 3290.6487\n",
            "Epoch[12/15], Step [280/469], Reconst Loss: 10686.6982, KL Div: 3240.2158\n",
            "Epoch[12/15], Step [290/469], Reconst Loss: 10349.8164, KL Div: 3191.5701\n",
            "Epoch[12/15], Step [300/469], Reconst Loss: 10836.0918, KL Div: 3255.8081\n",
            "Epoch[12/15], Step [310/469], Reconst Loss: 10115.3848, KL Div: 3197.7964\n",
            "Epoch[12/15], Step [320/469], Reconst Loss: 10253.0488, KL Div: 3178.9590\n",
            "Epoch[12/15], Step [330/469], Reconst Loss: 10195.7734, KL Div: 3162.4341\n",
            "Epoch[12/15], Step [340/469], Reconst Loss: 10077.4961, KL Div: 3314.8770\n",
            "Epoch[12/15], Step [350/469], Reconst Loss: 10269.2178, KL Div: 3167.7371\n",
            "Epoch[12/15], Step [360/469], Reconst Loss: 10138.5195, KL Div: 3189.6472\n",
            "Epoch[12/15], Step [370/469], Reconst Loss: 10073.6807, KL Div: 3206.3237\n",
            "Epoch[12/15], Step [380/469], Reconst Loss: 10127.3906, KL Div: 3219.8755\n",
            "Epoch[12/15], Step [390/469], Reconst Loss: 10581.8848, KL Div: 3198.2065\n",
            "Epoch[12/15], Step [400/469], Reconst Loss: 10153.4482, KL Div: 3198.1997\n",
            "Epoch[12/15], Step [410/469], Reconst Loss: 10218.8994, KL Div: 3254.2900\n",
            "Epoch[12/15], Step [420/469], Reconst Loss: 10556.8184, KL Div: 3254.5474\n",
            "Epoch[12/15], Step [430/469], Reconst Loss: 9872.7334, KL Div: 3244.1809\n",
            "Epoch[12/15], Step [440/469], Reconst Loss: 10267.4961, KL Div: 3260.6897\n",
            "Epoch[12/15], Step [450/469], Reconst Loss: 10195.1289, KL Div: 3119.8591\n",
            "Epoch[12/15], Step [460/469], Reconst Loss: 10144.0303, KL Div: 3238.6445\n",
            "Epoch[13/15], Step [10/469], Reconst Loss: 9897.0459, KL Div: 3174.9663\n",
            "Epoch[13/15], Step [20/469], Reconst Loss: 10279.9268, KL Div: 3181.4932\n",
            "Epoch[13/15], Step [30/469], Reconst Loss: 10348.4531, KL Div: 3176.8467\n",
            "Epoch[13/15], Step [40/469], Reconst Loss: 10149.1611, KL Div: 3339.4302\n",
            "Epoch[13/15], Step [50/469], Reconst Loss: 10494.5488, KL Div: 3085.4126\n",
            "Epoch[13/15], Step [60/469], Reconst Loss: 10187.0732, KL Div: 3207.9495\n",
            "Epoch[13/15], Step [70/469], Reconst Loss: 10398.5459, KL Div: 3272.9980\n",
            "Epoch[13/15], Step [80/469], Reconst Loss: 10076.1416, KL Div: 3185.7310\n",
            "Epoch[13/15], Step [90/469], Reconst Loss: 10350.1523, KL Div: 3268.3206\n",
            "Epoch[13/15], Step [100/469], Reconst Loss: 10677.1348, KL Div: 3323.4189\n",
            "Epoch[13/15], Step [110/469], Reconst Loss: 9929.0615, KL Div: 3150.7349\n",
            "Epoch[13/15], Step [120/469], Reconst Loss: 10452.6660, KL Div: 3329.1372\n",
            "Epoch[13/15], Step [130/469], Reconst Loss: 10440.0264, KL Div: 3112.9487\n",
            "Epoch[13/15], Step [140/469], Reconst Loss: 10265.5381, KL Div: 3311.9104\n",
            "Epoch[13/15], Step [150/469], Reconst Loss: 10429.9316, KL Div: 3243.7820\n",
            "Epoch[13/15], Step [160/469], Reconst Loss: 10179.1914, KL Div: 3117.9517\n",
            "Epoch[13/15], Step [170/469], Reconst Loss: 10271.5723, KL Div: 3340.9915\n",
            "Epoch[13/15], Step [180/469], Reconst Loss: 10382.5635, KL Div: 3196.0273\n",
            "Epoch[13/15], Step [190/469], Reconst Loss: 10173.2568, KL Div: 3217.9749\n",
            "Epoch[13/15], Step [200/469], Reconst Loss: 10500.6377, KL Div: 3350.6575\n",
            "Epoch[13/15], Step [210/469], Reconst Loss: 10545.6426, KL Div: 3171.8352\n",
            "Epoch[13/15], Step [220/469], Reconst Loss: 10349.7832, KL Div: 3224.4346\n",
            "Epoch[13/15], Step [230/469], Reconst Loss: 10171.6904, KL Div: 3308.8040\n",
            "Epoch[13/15], Step [240/469], Reconst Loss: 10187.8857, KL Div: 3121.6602\n",
            "Epoch[13/15], Step [250/469], Reconst Loss: 10896.1064, KL Div: 3332.8279\n",
            "Epoch[13/15], Step [260/469], Reconst Loss: 9979.1504, KL Div: 3169.9099\n",
            "Epoch[13/15], Step [270/469], Reconst Loss: 9935.0645, KL Div: 3280.1606\n",
            "Epoch[13/15], Step [280/469], Reconst Loss: 10217.9033, KL Div: 3190.8037\n",
            "Epoch[13/15], Step [290/469], Reconst Loss: 10483.6250, KL Div: 3313.9512\n",
            "Epoch[13/15], Step [300/469], Reconst Loss: 10170.0332, KL Div: 3262.9756\n",
            "Epoch[13/15], Step [310/469], Reconst Loss: 10443.9824, KL Div: 3267.4724\n",
            "Epoch[13/15], Step [320/469], Reconst Loss: 10149.1094, KL Div: 3266.6555\n",
            "Epoch[13/15], Step [330/469], Reconst Loss: 10294.3496, KL Div: 3280.5935\n",
            "Epoch[13/15], Step [340/469], Reconst Loss: 10806.7646, KL Div: 3404.0493\n",
            "Epoch[13/15], Step [350/469], Reconst Loss: 10142.1152, KL Div: 3235.7656\n",
            "Epoch[13/15], Step [360/469], Reconst Loss: 10442.4766, KL Div: 3348.9409\n",
            "Epoch[13/15], Step [370/469], Reconst Loss: 10838.9287, KL Div: 3284.7183\n",
            "Epoch[13/15], Step [380/469], Reconst Loss: 10585.9209, KL Div: 3200.6885\n",
            "Epoch[13/15], Step [390/469], Reconst Loss: 10052.7373, KL Div: 3184.3291\n",
            "Epoch[13/15], Step [400/469], Reconst Loss: 10029.3818, KL Div: 3124.4644\n",
            "Epoch[13/15], Step [410/469], Reconst Loss: 10389.8535, KL Div: 3205.0376\n",
            "Epoch[13/15], Step [420/469], Reconst Loss: 10266.1191, KL Div: 3239.3472\n",
            "Epoch[13/15], Step [430/469], Reconst Loss: 10357.5947, KL Div: 3143.1404\n",
            "Epoch[13/15], Step [440/469], Reconst Loss: 10016.3984, KL Div: 3165.9402\n",
            "Epoch[13/15], Step [450/469], Reconst Loss: 10759.2988, KL Div: 3366.5737\n",
            "Epoch[13/15], Step [460/469], Reconst Loss: 10146.5625, KL Div: 3305.5681\n",
            "Epoch[14/15], Step [10/469], Reconst Loss: 10271.5166, KL Div: 3163.1077\n",
            "Epoch[14/15], Step [20/469], Reconst Loss: 9731.6826, KL Div: 3112.0288\n",
            "Epoch[14/15], Step [30/469], Reconst Loss: 10593.5801, KL Div: 3287.9146\n",
            "Epoch[14/15], Step [40/469], Reconst Loss: 10217.8525, KL Div: 3297.9211\n",
            "Epoch[14/15], Step [50/469], Reconst Loss: 9835.9609, KL Div: 3146.2305\n",
            "Epoch[14/15], Step [60/469], Reconst Loss: 10093.4658, KL Div: 3207.4292\n",
            "Epoch[14/15], Step [70/469], Reconst Loss: 10189.3057, KL Div: 3221.2510\n",
            "Epoch[14/15], Step [80/469], Reconst Loss: 10614.0449, KL Div: 3291.4656\n",
            "Epoch[14/15], Step [90/469], Reconst Loss: 10421.3340, KL Div: 3215.1660\n",
            "Epoch[14/15], Step [100/469], Reconst Loss: 10064.4785, KL Div: 3249.0457\n",
            "Epoch[14/15], Step [110/469], Reconst Loss: 10440.2148, KL Div: 3321.9084\n",
            "Epoch[14/15], Step [120/469], Reconst Loss: 9820.1230, KL Div: 3182.7991\n",
            "Epoch[14/15], Step [130/469], Reconst Loss: 10012.2217, KL Div: 3205.2253\n",
            "Epoch[14/15], Step [140/469], Reconst Loss: 10275.1475, KL Div: 3250.6846\n",
            "Epoch[14/15], Step [150/469], Reconst Loss: 10277.8760, KL Div: 3228.1504\n",
            "Epoch[14/15], Step [160/469], Reconst Loss: 11062.4141, KL Div: 3204.1079\n",
            "Epoch[14/15], Step [170/469], Reconst Loss: 10145.8262, KL Div: 3232.6492\n",
            "Epoch[14/15], Step [180/469], Reconst Loss: 9943.1934, KL Div: 3202.1265\n",
            "Epoch[14/15], Step [190/469], Reconst Loss: 10309.6543, KL Div: 3355.4258\n",
            "Epoch[14/15], Step [200/469], Reconst Loss: 10655.4785, KL Div: 3220.8718\n",
            "Epoch[14/15], Step [210/469], Reconst Loss: 10181.6729, KL Div: 3156.1873\n",
            "Epoch[14/15], Step [220/469], Reconst Loss: 10041.7607, KL Div: 3293.1123\n",
            "Epoch[14/15], Step [230/469], Reconst Loss: 10240.5742, KL Div: 3203.2847\n",
            "Epoch[14/15], Step [240/469], Reconst Loss: 10614.5889, KL Div: 3252.9194\n",
            "Epoch[14/15], Step [250/469], Reconst Loss: 10249.5840, KL Div: 3295.1328\n",
            "Epoch[14/15], Step [260/469], Reconst Loss: 9964.4111, KL Div: 3121.4712\n",
            "Epoch[14/15], Step [270/469], Reconst Loss: 10286.4844, KL Div: 3277.7556\n",
            "Epoch[14/15], Step [280/469], Reconst Loss: 10063.9648, KL Div: 3243.0161\n",
            "Epoch[14/15], Step [290/469], Reconst Loss: 10054.7754, KL Div: 3177.9180\n",
            "Epoch[14/15], Step [300/469], Reconst Loss: 10374.7734, KL Div: 3294.9565\n",
            "Epoch[14/15], Step [310/469], Reconst Loss: 10244.4932, KL Div: 3068.2437\n",
            "Epoch[14/15], Step [320/469], Reconst Loss: 10263.8311, KL Div: 3219.4673\n",
            "Epoch[14/15], Step [330/469], Reconst Loss: 10525.3877, KL Div: 3219.4160\n",
            "Epoch[14/15], Step [340/469], Reconst Loss: 10275.4629, KL Div: 3195.4727\n",
            "Epoch[14/15], Step [350/469], Reconst Loss: 9840.3662, KL Div: 3160.6196\n",
            "Epoch[14/15], Step [360/469], Reconst Loss: 10184.0264, KL Div: 3185.3906\n",
            "Epoch[14/15], Step [370/469], Reconst Loss: 9924.4365, KL Div: 3173.0315\n",
            "Epoch[14/15], Step [380/469], Reconst Loss: 10674.6836, KL Div: 3279.4932\n",
            "Epoch[14/15], Step [390/469], Reconst Loss: 10150.4824, KL Div: 3172.5210\n",
            "Epoch[14/15], Step [400/469], Reconst Loss: 10767.8398, KL Div: 3312.0537\n",
            "Epoch[14/15], Step [410/469], Reconst Loss: 10187.5244, KL Div: 3243.2229\n",
            "Epoch[14/15], Step [420/469], Reconst Loss: 10073.7754, KL Div: 3169.2219\n",
            "Epoch[14/15], Step [430/469], Reconst Loss: 10454.1152, KL Div: 3237.8110\n",
            "Epoch[14/15], Step [440/469], Reconst Loss: 10187.8418, KL Div: 3193.9026\n",
            "Epoch[14/15], Step [450/469], Reconst Loss: 10353.8301, KL Div: 3225.3984\n",
            "Epoch[14/15], Step [460/469], Reconst Loss: 10466.5820, KL Div: 3259.5061\n",
            "Epoch[15/15], Step [10/469], Reconst Loss: 10318.0791, KL Div: 3285.0054\n",
            "Epoch[15/15], Step [20/469], Reconst Loss: 10176.6055, KL Div: 3243.9829\n",
            "Epoch[15/15], Step [30/469], Reconst Loss: 10209.2354, KL Div: 3336.6772\n",
            "Epoch[15/15], Step [40/469], Reconst Loss: 10029.5352, KL Div: 3242.8560\n",
            "Epoch[15/15], Step [50/469], Reconst Loss: 10131.0508, KL Div: 3210.6514\n",
            "Epoch[15/15], Step [60/469], Reconst Loss: 10466.3799, KL Div: 3240.4194\n",
            "Epoch[15/15], Step [70/469], Reconst Loss: 10163.6865, KL Div: 3219.2056\n",
            "Epoch[15/15], Step [80/469], Reconst Loss: 9819.1318, KL Div: 3257.1003\n",
            "Epoch[15/15], Step [90/469], Reconst Loss: 10313.7607, KL Div: 3246.3120\n",
            "Epoch[15/15], Step [100/469], Reconst Loss: 10331.9902, KL Div: 3227.1375\n",
            "Epoch[15/15], Step [110/469], Reconst Loss: 9626.7246, KL Div: 3235.6836\n",
            "Epoch[15/15], Step [120/469], Reconst Loss: 10027.0469, KL Div: 3171.5107\n",
            "Epoch[15/15], Step [130/469], Reconst Loss: 10086.3145, KL Div: 3245.7900\n",
            "Epoch[15/15], Step [140/469], Reconst Loss: 10291.5645, KL Div: 3295.9153\n",
            "Epoch[15/15], Step [150/469], Reconst Loss: 10345.0342, KL Div: 3271.1384\n",
            "Epoch[15/15], Step [160/469], Reconst Loss: 10022.8877, KL Div: 3204.5862\n",
            "Epoch[15/15], Step [170/469], Reconst Loss: 10805.5205, KL Div: 3285.5615\n",
            "Epoch[15/15], Step [180/469], Reconst Loss: 10217.2686, KL Div: 3268.8726\n",
            "Epoch[15/15], Step [190/469], Reconst Loss: 10269.1016, KL Div: 3205.6013\n",
            "Epoch[15/15], Step [200/469], Reconst Loss: 9989.0781, KL Div: 3246.7559\n",
            "Epoch[15/15], Step [210/469], Reconst Loss: 10089.7637, KL Div: 3240.1760\n",
            "Epoch[15/15], Step [220/469], Reconst Loss: 10149.7383, KL Div: 3178.2563\n",
            "Epoch[15/15], Step [230/469], Reconst Loss: 10215.9453, KL Div: 3161.2268\n",
            "Epoch[15/15], Step [240/469], Reconst Loss: 10211.9268, KL Div: 3265.1892\n",
            "Epoch[15/15], Step [250/469], Reconst Loss: 10145.9092, KL Div: 3299.2913\n",
            "Epoch[15/15], Step [260/469], Reconst Loss: 10710.8770, KL Div: 3313.4043\n",
            "Epoch[15/15], Step [270/469], Reconst Loss: 10657.1436, KL Div: 3335.4478\n",
            "Epoch[15/15], Step [280/469], Reconst Loss: 10550.9980, KL Div: 3235.6919\n",
            "Epoch[15/15], Step [290/469], Reconst Loss: 9825.5723, KL Div: 3296.8306\n",
            "Epoch[15/15], Step [300/469], Reconst Loss: 9953.8740, KL Div: 3245.3313\n",
            "Epoch[15/15], Step [310/469], Reconst Loss: 10698.3193, KL Div: 3240.6377\n",
            "Epoch[15/15], Step [320/469], Reconst Loss: 10267.3428, KL Div: 3245.6936\n",
            "Epoch[15/15], Step [330/469], Reconst Loss: 10151.6680, KL Div: 3251.9414\n",
            "Epoch[15/15], Step [340/469], Reconst Loss: 10212.1855, KL Div: 3201.6868\n",
            "Epoch[15/15], Step [350/469], Reconst Loss: 10373.2197, KL Div: 3174.8179\n",
            "Epoch[15/15], Step [360/469], Reconst Loss: 10288.9141, KL Div: 3280.3640\n",
            "Epoch[15/15], Step [370/469], Reconst Loss: 9957.1846, KL Div: 3266.6416\n",
            "Epoch[15/15], Step [380/469], Reconst Loss: 10233.4346, KL Div: 3200.6812\n",
            "Epoch[15/15], Step [390/469], Reconst Loss: 10657.1387, KL Div: 3180.5105\n",
            "Epoch[15/15], Step [400/469], Reconst Loss: 9993.2471, KL Div: 3251.4307\n",
            "Epoch[15/15], Step [410/469], Reconst Loss: 10768.2363, KL Div: 3261.2522\n",
            "Epoch[15/15], Step [420/469], Reconst Loss: 10135.9541, KL Div: 3164.2664\n",
            "Epoch[15/15], Step [430/469], Reconst Loss: 9705.2920, KL Div: 3210.5020\n",
            "Epoch[15/15], Step [440/469], Reconst Loss: 10285.2070, KL Div: 3158.6377\n",
            "Epoch[15/15], Step [450/469], Reconst Loss: 10183.2139, KL Div: 3315.2627\n",
            "Epoch[15/15], Step [460/469], Reconst Loss: 10606.0439, KL Div: 3247.4609\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kDjCST16CEx2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8e75aa09-d6b0-42ef-b873-eeb7f54faa14"
      },
      "cell_type": "code",
      "source": [
        "# Save the sampled images\n",
        "z = torch.randn(10, z_dim).to(device)\n",
        "out = model.decode(z).view(-1, 1, 28, 28)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "BdRcUB3-ZzLs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "c0440766-242b-407d-97ee-f6da0e699550"
      },
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(2,5, figsize=(15,5))\n",
        "for i in range(10):\n",
        "  ax[int(i/5), i%5].imshow(out[i,0].cpu().detach().numpy())\n",
        "  ax[int(i/5), i%5].axis('off')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1AAAAEwCAYAAAC5cJBCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XnQ3VV5OPCDkI3sJCF7SMhCWGQR\niEShUERKKVtby4wz2k6Z4nS62HZEO1hbGEWnrZ0yap2xM4pTqzI4bcWRKjDsGNZAs7GFgCSEEJKQ\n5c0KCfL7o78/ep5zzD2+hOS97/v5/Pfcec69980993vvyT3PeQ57++23304AAAB09J5D/QQAAAC6\nhQUUAABAIwsoAACARhZQAAAAjSygAAAAGllAAQAANLKAAgAAaGQBBQAA0MgCCgAAoJEFFAAAQCML\nKAAAgEYWUAAAAI0soAAAABpZQAEAADSygAIAAGhkAQUAANDoiEP9BPqit99+u7ht3759WfzWW28V\nOe95T74erd1PdNhhh+33PlJK6fDDD9/vGAAA4ODwCxQAAEAjCygAAIBGFlAAAACN1EClslYp1jul\nlNLmzZuz+MEHHyxyHnvssSz+xS9+kcWjR48uxkyZMiWLTz/99CJnzpw5WTxs2LAip1Y79X+pmwKA\ndyZ+rqeU0pYtW7L4e9/7XpEza9asLJ42bVoWz5s3rxgzdOjQLK59zvtsh0PDL1AAAACNLKAAAAAa\nWUABAAA0soACAABo5BCJVDbFff3114ucr33ta1l86623FjmxkDQeThELQlNK6cQTT8zibdu2FTmD\nBw/O4hkzZhQ5Rx55ZBZ3OlQCfpmWBtCRQmYGqvh+8V7oXrVr3+7du7P4T/7kT4qcm2++OYvffPPN\nIifOi1GjRmXx3LlzizEf+tCHsvgv//Ivi5wJEyZk8eGHH17kQCe1ub93794s3rRpUxa//PLLxZj4\nfXrmzJlFzqRJk7K4W7+vduezBgAAOAQsoAAAABpZQAEAADRSA1WxcePG4rYXX3wxi+O+6JRSGjFi\nRBZPnTo1i2ND3JRSmjhxYhZv2LChyLnjjjuy+MwzzyxyYgPeWG9lX37/V9vDHJs+7tmzp8iJcy42\nja41gI63DR8+vMiJcy7ujU4ppSOOOGK/cW1vdLful6bvi03U16xZU+S89NJLWXzUUUdlcWyOnlJK\n48aNy+La9Tje5prdN/T09GTx0qVLi5x4bau9drFOOdaGnHHGGcWY+J1i0aJFRU6skxo5cmSRoy5q\nYInfBWqfvdEbb7xR3LZixYos/slPfpLFq1atKsasW7cui+N33JTKMwViHV+38E0EAACgkQUUAABA\nIwsoAACARhZQAAAAjQ57uzddM/uZWGAXm4WlVBZv7tixo8hZsGBBFsdi4lggn1LZqOyuu+4qcr7z\nne/sd0xKKX3qU5/K4vPOOy+LFZH2f7VC0XhoRK0oftmyZVk8aNCgLK41hYxNIIcNG1bkxALTJ598\nssiJh09ceeWVWXzCCScUYzSNpjfiR13tsKDYqPT2228vcuLBLLGI/5prrinGxEN+ap8FDpHom+J1\ntfbZH4vtjz766CJn8uTJWdxy0FP8rF+/fn3Hxz7uuOOKnHhgRbzG071qn/tx3tS+/8XPzXhdS6n8\n7I+f6TfccEMx5sEHH8ziIUOGFDnf/e53s/jss88ucrrh+uebBwAAQCMLKAAAgEYWUAAAAI000k3l\nXtDYvC6lcg/7mDFjipxYz9FSdxT3Itfu96mnnsri2h7sW2+9NYvjnlI1UP1PS5PclStXZvHTTz9d\n5Nx22237vd/afvnYfHfXrl1FTqwxqc3buMf6+eefz+LPfe5zxZgTTzwxi2v1V92wf7rbxZqig/lv\nHh87xrW6vXvuuSeLY91oSmXD9Nr9xPdDrHc59thjizHx+tvSSJe+Ib52sf4zpZTOOuusLK7VZfbm\n9Y33U/t+sHv37iyOjZ5TKhs5jx079h0/Nw6NWPO0ZcuWIid+h22puax9R4zj5s+fn8Wf/exnizHX\nX399FsfGuinV66K6kV+gAAAAGllAAQAANLKAAgAAaKQGqqK2FzTuGa7VXbTsc49iTu0s/tiXqna/\nsf7Knub+pdauLdZnPPvss0VOrG+K9XQppXTvvfdmcaylqj123Btd29Mca0XiPvyUUho8eHAWx/qR\nqVOndhxD33CgWgp2qm9Kqexz8sILL2TxD3/4w2LMQw89lMVvvPFGkTNy5MgsjtfVlMp+f5/85Cez\nePz48cUYNaj927vViy5+jsceeCmlNHHixCzu6ekpcrZu3ZrFsY6rViND3xC/E27fvj2L9+3bV4yJ\nr2dvvw/GcfFzPtZEpVT2xat95+gvn+F+gQIAAGhkAQUAANDIAgoAAKCRBRQAAEAjlYO91JtGiLVi\n6FgA+OSTT3a833igRUopXXTRRVmsaLl/ic3zUiobg37rW98qch544IEsrhUY14pQO4nF9scdd1yR\n8+u//utZfOaZZxY5U6ZMyeLp06dncW2u9+awFt59LYc/xNtq8zoejhKLplMqD41YsmRJx+d3xRVX\nZHHt4JM1a9ZkcZznKaV0+eWXZ3Gcs+/WgQL0XXEeH6hGui3vqdjIuXbAVRSv+S3P13X20OjUsHz0\n6NHFmL70/W/SpEnFbbUDpbqRKz0AAEAjCygAAIBGFlAAAACN1EBV1Pb67ty5s2NObA4W9xXHBqUp\npXTfffdl8S233NLx+cyePbvIibfZr9zdYvO8l156qcj5/Oc/n8X/8z//U+TEhqO1PfSd9rrXmixe\ncsklWXzNNdcUOTNmzMjiWhPIuFc7vmcOVC0B70xLPVOca7VGtfEauG7duiIn1kBt2LChyIn3ffrp\np2dxrXZuzJgxWbx79+4i5+GHH87iOXPmFDlxT7+aPH7+859nca1R6FFHHZXFQ4cOzeLatS7WKm3Z\nsqXIifVXtfqS+Nj0TbXrbJwXsQlyzcG6Bm3evLm4LdannnTSSUVOrW6rG/kFCgAAoJEFFAAAQCML\nKAAAgEYWUAAAAI0cIlFRK35+/PHHs/juu+8ucnbt2pXFI0aMyOJXX321GLNs2bIs3rp1a5ETC+4W\nLFhQ5NQaPtI9YvFoLHD/8z//82LM4sWLs7jWlLRFLDiNRfG1ouTYlDQeGJFS2ai01twvFshq3tg9\nYoF7vP698sorxZh4SMOOHTuKnHhIw7Rp04qciRMnZnFsJlp7L8Tnu3HjxiJn+fLlWXz88ccXObVD\nVRg4aoX+cV7ffPPNRU48NOK0007L4trhUPHa23JAxIQJE4qclmsxh15f/7yL19BvfetbRU58f7z/\n/e8vcuJ87Ot/9y/jFygAAIBGFlAAAACNLKAAAAAa2cydysaNP/zhD4ucf/qnf8ri2DgvpbJ2Ku55\njnFKZXPRWgO+6dOnZ/GsWbOKnEGDBmVxbMRa22ParftO+6O4b3jTpk1ZHOudUipf4xa1ve9x7rTM\n27j3+Wc/+1mRM3fu3Cw+77zzipyZM2d2fCwOvVrdR5x/scnnT3/602LMihUrsrh2LYu1IJMnTy5y\nYs1nfH61OtZ4zX7ooYeKnNiAt/bYtYanDGzjx4/P4lr9X5x/d9xxRxbHupCUyjqpc889t8i56qqr\nsjhez1PqXLfn+wE18br67LPPZvGDDz5YjPmd3/mdLI6f8SnV53o38kkAAADQyAIKAACgkQUUAABA\nIwsoAACARgPuEIlag8XYzPZv//Zvi5x169Z1vJ9YoB8b6daaMsZDIzZs2FDkxML6vXv3Fjl79uzZ\n7/3St8WC3ViUfOGFFxZjbr311iyOTe5SKovtzz777CInFsrHYudVq1YVY5588sksXrRoUZET3w+1\nAwO+8pWvZPEHP/jBIodDr6WgPM6bRx55pMjZvHlzFtcONYm31Q5LGTNmTBb39PRk8dq1a4sx8WCW\n7du3FzmXXHJJFsdDJVLS7Hmgq73ep556ahbPmzevyHn55ZezOB50UjucJL6naoeaRLUDIzrNUXOY\nmnhdveCCC7K4dv2+9tprs3j48OEH/on1EX6BAgAAaGQBBQAA0MgCCgAAoFG/r4GKjcB27dpV5Pz9\n3/99Fse98imVzelik9CUUrr66quz+Dd+4zc6Pr8HHnggi3/84x8XOevXr8/iWgPIc845J4tj7Qt9\nW9yDHhssf/vb3y7GfOMb38jiWgPFWAtX2+sea0xibcj9999fjLn++uuz+Omnny5yYq3eU089VeSs\nWbMmi2s1WnSH+HrX6jBjTmy+m1LZ2LyWE+d6nLOvvfZaMSbu1z/ppJOKnNj0sTf1JPRvtdd/woQJ\nWXzTTTcVObEJ6Te/+c0s3rp1azEmXh9rdVK1muhOzGGiWrP0f/mXf8nieF2t1frNmDEji/tz4/H+\n+5cBAAAcYBZQAAAAjSygAAAAGvW7Gqi4jzP2WnjiiSeKMXGfcezBk1JZm/GFL3yhyIl7P+M+41qf\nnrjff/HixUXOihUrsvjRRx8tcmIt1fTp07PYXv7uEl+bWj3Jger1FWtDYo+dyy67rBhz5plnZvH8\n+fOLnDi3a/P/xBNPzGJzsnvEeRNfy49+9KPFmNhPryZeq2q9mGLdx6uvvprF27ZtK8bs3Lkzi8eN\nG1fkxLlf63NijhLFOVurQY7XzJNPPjmLa/32Yi1VbT7GuT9t2rQipzYO/q/a5/Mtt9ySxbH2dMGC\nBcWYWA/Yn/kFCgAAoJEFFAAAQCMLKAAAgEYWUAAAAI26+hCJWuOvWFy8bNmyLP7+979fjBkxYkQW\nL1y4sMj5q7/6qyyeOnVqkRMLNePzqxUfx+Z5PT09RU6tQWr0yiuv7Pd+YmPW2vNRHE1NbV5MnDgx\ni2vzKzY3rd3P8ccf/w6fHQdD7bWL17tJkyZl8Yc//OFiTGzaHOPa/dYeOx4OFA/RqR3GE6+j5557\nbpEzfPjwjo/NwFb73tGbeRIPAYqHUKWU0nnnnZfFtWa7tWsv/Kqefvrp4rZ46M8JJ5yQxXF+pnTg\nDrfqBn6BAgAAaGQBBQAA0MgCCgAAoFFX1UDFvce7d+8ucuI+zn/+53/O4uXLlxdjYtO7yZMnFzmx\nTqq2dz82Do31WLFOKaWUFi1alMW1vdRxb/Rbb71V5MS/ITaNrDVJqzXXhRax6ePrr7/ecczo0aOL\n2wbSfun+5j3vyf//LV67ent9idf5Ws1JrIGKzdDjtTillM4555wsPuaYY4ocDUf7t05zq/a5Hh3M\nORLfU7Xa63hdVbdHi3gNvfbaa4ucOJd+67d+K4svu+yyYsxA+l7pFygAAIBGFlAAAACNLKAAAAAa\nWUABAAA06qpqr3gQwrPPPlvk3HDDDVl81113ZXGtIDkeEPHYY48VOePHj8/i2Eg0pZS2bduWxbFI\nr9YE77XXXsvi2gEWsVHeuHHjipxYXDps2LAsjgXf9C0tTZcPldphLe973/uyuHZISfQXf/EXxW19\n6e/knenNa1m7HkfxOppSSo888kgW33vvvVl82mmnFWMuvvjiLG5pLk73qh0IsWHDhiz+z//8zyyO\n3wVSSumSSy7J4rFjxxY5B2Le1K6hPT09WVw7iCfO49pnvXk9sNWus9/85jezeMmSJUVOnG9XXHFF\nFsfDywYa36oBAAAaWUABAAA0soACAABo1GdroGp7NmNj2tqezaVLl2bxrl27Oj5WvN/HH3+8yPn5\nz3+excOHDy9yhgwZksWxbqrWuDHuua41JI17nI8++ugi59hjj83iuE+71tzMvuhDo7Y3PzZHjvO/\n9vq9W3Vtseap1ryx5X01aNCgLP7rv/7rd/bEGBBiE9wnnniiyIm1K/GaGBs+plRefzXN7d/i53pK\nKX3mM5/J4ltvvTWLWz5bP/CBDxQ5LTWsMSfO89r3mfvvvz+LZ8+eXeSccsopHR+bgS02Gk8ppX//\n93/P4vh5nVJKl156aRbPmjUriw/m95K+aOD8pQAAAO+QBRQAAEAjCygAAIBGFlAAAACN+uwhEjWx\nyO3EE08scuJtsVFtLNZPqSyEqxXGxYLUWnPR2PAxHuRQa4C7c+fOLI6FpSmVDfZqxbHxwIpYSKqw\ntO+ozcFXXnkli5955pksjoXCKZWvea0INL7utcNZ4mOfcMIJWbx9+/ZiTFQrHI1NrOMhKww8cf7V\nmofGZqd33313kRPn5OWXX57F8+fPL8bUrusMLHHexM/fWrH917/+9Syufe+oNWWOtm7dmsU33nhj\nFv/0pz8txuzZsyeLTz311CJn8ODBWeyznvgd4/rrry9ynn/++SyO3ydSKhuSx8PTagfxDKT55xco\nAACARhZQAAAAjSygAAAAGh32dq0ooo+KDUjj/uCUUnrxxRez+Ac/+EEWx32fKaU0YcKELJ4yZUqR\nE+uXYm1VSmUT3Nj07r3vfW8xZt26dVkca19SKveZzp07t8iJNTIzZ87M4mHDhhVjBlLDs76kVsP2\n5JNPZvGXvvSlLF61alUx5qKLLsriq6++usjZsWNHFv/Zn/1ZkbN48eIsrtVoRbGeJN5HSvW6Lfqv\n+FFS+2iJ1/A4P1NK6Uc/+lEWL1u2rMiJzUwvvPDCLK41One9G1hq82/lypVZfO6552ZxrL9LqbzW\nzZgxo8iJNXfxu0BK5XeGnp6eLK5dd2O91S233FLkxO8rA6kGhbrVq1dn8YIFC4qcLVu2ZHGttu/m\nm2/O4nnz5mXxQL+mDuy/HgAA4FdgAQUAANDIAgoAAKBRV9VARbWnHvcR1/qMdFLbQxxvq9WxxD5Q\nsT9DrQ9J7CdVq62KvSpq/aTGjBmTxbEvRe2xB/r+1UMl1oGkVNY4ffzjH8/iFStWFGPi3K71ZIjz\ntOX9EOf69OnTi5zly5dn8ahRozreL/1Lp5qnWk+72AvnzjvvLHJinco555xT5MQaqFjz5NpGTbz2\nPvfcc1n86U9/uhjzwAMPZHHtGhrnW62eKfbpmzZtWhZfeeWVxZhPfOITWTxp0qSOj83AUvseHD+f\nzz///CInzuPf+73fK3Jir7J4nR3o9XbeeQAAAI0soAAAABpZQAEAADSygAIAAGjU1YdI9DUH4p+y\nVnwab2spGo0HCrQcjMGhEws6lyxZksW///u/X4x54YUXsrh2sEnLnIwHjFxzzTVZ/MUvfrEYo3CZ\nWJAfD42ITcJTSun222/P4nvvvbfImTx5chZ/8pOfLHJio/DaITnwq6od8BPn9ebNm4ucl156KYtr\njeuPPfbY/eY46IneqH3Gx4bQtc/weIDZZz/72SLnmGOOyWLzMedfAwAAoJEFFAAAQCMLKAAAgEZq\noKAPinVvq1evLnLinuXYPC+lck//b/7mbxY51113XRaPHj26+XkyMNQ+JuLc2rZtWxbfc889xZj/\n+q//yuJNmzYVOZdcckkW1+r/YuNme/MB/le8Xtdq6+M10zX0V+dfDAAAoJEFFAAAQCMLKAAAgEYW\nUAAAAI0cIgHAfrUcIhEbMz7yyCPFmOeffz6LFyxYUOTMnTs3i4cPH17kxEbhAHAw+QUKAACgkQUU\nAABAIwsoAACARmqgAHjH4kdJS93UYYcdVuTEho61HAA4lPwCBQAA0MgCCgAAoJEFFAAAQCMLKAAA\ngEYOkQAAAGjkFygAAIBGFlAAAACNLKAAAAAaWUABAAA0soACAABoZAEFAADQyAIKAACgkQUUAABA\nIwsoAACARhZQAAAAjSygAAAAGllAAQAANLKAAgAAaGQBBQAA0MgCCgAAoJEFFAAAQCMLKAAAgEYW\nUAAAAI0soAAAABpZQAEAADSygAIAAGhkAQUAANDIAgoAAKCRBRQAAEAjCygAAIBGFlAAAACNLKAA\nAAAaWUABAAA0soACAABoZAEFAADQyAIKAACg0RGH+gkAAAwkb7/9dhYfdthhh+iZAL3hFygAAIBG\nFlAAAACNLKAAAAAaWUABAAA0cogEAEDFL37xiyzet29fkbNr164sXrt2bZEzfvz4LF6zZk0Wjxo1\nqhhzzDHHZPHQoUOLHIdPwKHhFygAAIBGFlAAAACNLKAAAAAaqYE6iOJe6pRS6unpyeKNGzcWOcOH\nD8/ikSNHFjkjRozIYvuiAeCXi81sa5/Re/bsyeJYu5RSSo899lgW1z7HV65cud/Hvuqqq4oxM2bM\nKG6DgyW+H954440srtX6vfDCC1k8derUIifW9sXvuCmldPjhhzc/z0PFL1AAAACNLKAAAAAaWUAB\nAAA0soACAABo5BCJAygWhcaCu9tuu60Y88UvfjGLX3755SLniCPyl+kP/uAPipzrrrsui4888sj9\nP1n4/+K87Y2+dmhJy9/U154zvRdf77feeqvIiQXRtdc/Fi7XcjrNm96+n8zHgy/+m7/nPeX/Kcfb\nhgwZUuTcd999Wfzggw8WOXFeTJs2LYs3bdrU8fnBgRLnY2wGnVJKDz/8cBZ/4QtfyOJ4MEpKKe3e\nvTuLZ82aVeRcffXVWfyxj32syKk1lu5r/AIFAADQyAIKAACgkQUUAABAIzVQvVTb5x4b7sV90V/9\n6leLMbHmqbYPdfTo0Vk8adKkIifWSTHw1JpAxnm6b9++Imfv3r1ZHOdxrS5g6NChWVxrehdvq91P\nb/b4x7+pdh/x36L2fvWe6Zvia1V77eLrG+dwrD+tjRk0aFCRE2+rzZE432Jcex9GB+q9wIFVew2G\nDRuWxbF2KaWUzjvvvCxeunRpkfPaa69l8ZtvvpnFY8aMKcbEeVJ7L5g39EasVfr+979f5Hz5y1/O\n4nXr1mVxrdY0zsfVq1cXOc8888x+x3QLv0ABAAA0soACAABoZAEFAADQyAIKAACgkSrqRp2a5KaU\n0qJFi7L4P/7jP7J4/fr1xZhYhFcrWj722GOz+PLLLy9yagXR9B+14uE4d+LhDymltGHDhixesWJF\nkbNx48Ysnjx5chZPmTKlGBOb3MVC65RSGj58eBbXGlC2HDQRxX+LWtF+vK12yAWHXsu8bjn4JBYh\nb9++vRizc+fOLK5dMwcPHtzxsWPxdZzntYLoeEBA7bHjwSzdWljd39Veu4suuiiLa410Fy9enMXx\n4Injjz++42PV5kRvGjebWwNL7TPyRz/6URb/wz/8Q5ETDzW79NJLs3j+/PnFmDi3tm7dWuTE5rrx\nep5S22FRh5pfoAAAABpZQAEAADSygAIAAGikBqpR3KO5ZMmSIucb3/hGFq9cuTKL4x78lMo99rUa\nqIsvvjiLp0+fXuT0xf2h9F7c/1vbIxxr6mLj5pRSWr58eRavWbOmyImNms8444wsnjhxYjEm1pjU\n6vvGjh3b8X6OPPLILG6p5as174u8H/qmuBc/NhNNqazle/3114ucTZs27fd+Y+1fSik9/vjjWdxS\nt1erdV22bFkWxzqAc889txgzZ86cLI7vDbpH7doSr6GnnnpqkRObkMaGvCNGjGh6rE5q9S7xmhlr\nQmuP06lhNN2jdg397//+7yw+5phjipzf/d3fzeKPfexjWRzrNlMqa0RXrVpV5MTbanVSsW60L84/\nv0ABAAA0soACAABoZAEFAADQSA1URa2vQjwP/6abbipyYh+ouD+0tl807k2u7cs/55xzsrhWJ0V3\n61TztHnz5mLMT37ykyx+7LHHipzYV6nWt+HMM8/M4uOOOy6La/O2p6dnv883pbIOpTa3420tvR9a\nejq19JPiwIqvXe06GmueanWha9euzeLVq1cXOXfeeWcWx7k2cuTIYkycE+PHjy9y4t78Wg1U7GcW\nHyv2OKk9Vu0a3hf3+NMmzq3a9XrLli1ZHHvp9fZzPdY8tfTFi8/X3Otf4utdu4aOGzcui6+66qoi\n5+yzz87ilhrl+Pk8e/bsIie+F2o9IruBbxkAAACNLKAAAAAaWUABAAA0soACAABo5DSCilrx86OP\nPprFDz/8cMdxM2fOzOLYfDSlsrj4/e9/f5Fz1llnZbGCz/4vFoHGosuUymaitaL4eFutyeekSZOy\nOBY31wr9Y2O+WqO+2AivVtzcqVljba7X3p+d1B5bIfW7q/ZvHhuH18TmynEepZTSvHnzsjg2gawV\n5Mei6fg4KZWHUcydO7fIiU1SYxPVloNPHHLSv8TDUWKz5ZRSevXVV7N48uTJHe+35ZoUc2pzq9P9\n1N6rUe3wHtfMvil+Rtaaz0+ZMiWLzzjjjCInHhoRX++Wz+LBgwcXt8WDdmrX+G6YW67iAAAAjSyg\nAAAAGllAAQAANFIDVVHb1/nII49kcdzPnFK5Z7OlUV687e/+7u+KnNoeUvq3uN+8Vt/0oQ99KItX\nrlxZ5MT6pfXr13d8rFhb9dRTTxVjYg1grcFebABda6jXqeap9l7szd7o2phu2GPdl/WmFq3TnvqU\nUjr66KOzeMKECUVO3L8fazhqdXsjRozo+PziY9XGxPdLy5ztzb8V3SM2f77vvvuKnF27dmXxAw88\nkMW1+tSWxuItNVCd7rdWmxjrZlru1zW1b4ivQ+07ZPw8Hjp0aMf7aRHnVq12burUqR2fXzfMJb9A\nAQAANLKAAgAAaGQBBQAA0MgCCgAAoJFDJCpqxWuvvfZaFr/xxhsdx61atSqLawXJF1xwQRaPHTu2\n+XnSf8S5Ew8Xqc2Lk046KYvjoSUplfP2xRdfLHLuv//+LI4Nn1955ZViTGw4evLJJxc5e/fuzeIh\nQ4YUObEwuTdF070t2m95LP5Xb/49a+LrXStMjwXFtSafsQlubGRaK0qOB7HU5uOBaHjb0vz5QB2O\nwsFXe+2++tWvZnGt8Xm8ptcaOffGgThUp3Yfna7NB+q5cODF16F2EE+cj7XrbKcGy/EzPqVyntSu\nofGxu7WxeHc+awAAgEPAAgoAAKCRBRQAAEAjNVAVtX28cb98y37Rbdu2ZXFsHpZSShdffHEWd+te\nUA6sTjVRKaU0fPjwLJ47d26RM3369P2OSSmlO++8M4tj7V6tyeKsWbOyuNYEMjbSHTlyZJHTaS9+\ny5763uZobvrOxEabLf+e8RoZ7yOlsp6p1hS3p6cni+O1Ns77lMq6qNp76t2q4Yh/d+1xag0n6Xu2\nbt1a3Pbd7343i2vvhaOOOiqLL7vssiw+mJ/9LfWfLfPR95XuUGt0H+v4N23aVOTE2us4T2ItdEop\nDRs2LItrtdm159ONzH4AAICDgGF1AAAMkklEQVRGFlAAAACNLKAAAAAaWUABAAA0cohEo1hQWWsg\nFguiN27cmMXLly8vxsRGp9AqFvDWCjNjzuTJk4uceNvixYuzeNy4ccWYhQsXZvH5559f5MTmfbWi\n/fj8enOIRI0DIt6ZlsMedu/encVx/tVeu1i4XGvSvHLlyv2OqT1WPCBizpw5xZhDWfBuPnaveKhJ\nbGCeUko7duzI4tpcmz17dhbH7xS1w3riNbOlsXgLTcRZv359Fq9evbrIefXVV7P4rrvuyuLade2P\n//iPs7h2wFR/mW9+gQIAAGhkAQUAANDIAgoAAKCRGqiK2n7/u+++u2NOrBuIe5pre/mfeeaZLL7i\niiuKnP6yX5R3V22edKoxSqms1Yvzttag9wMf+EAWjx8/vsiJ+/drdQEHquap0/32Nof/9frrrxe3\nxdqQ2Ci0pRFnvI+UyobLsWlzSmV93Z49e7K4Vk8SbzuYjWtb3occerXP9T/8wz/M4nXr1nW8n9r1\n8Ld/+7ezOM6J7du3F2Na6lyHDBmy3zEplfOtpSav5dqslqo77Nq1q7jtzjvvzOIlS5YUOStWrMji\n+B12xowZxZg77rgji0844YQiJzY679aGzN35rAEAAA4BCygAAIBGFlAAAACN1EBVxLPvU0rpueee\ny+JY71QT9wOPGDGiyIn3E3urpJTSkUceud/7hV8mzq+f/exnRc4999yTxXF+TZw4sRhT6ycVqUPq\nDrV6iFgv9J3vfKfIufDCC7P46KOPzuJavUa8rbY/Ps6Jlv3xsX4k1vWlVL4XavO6pe9OJ7V/z3er\n1o93Js6JZ599tsiJtSK1eR3r9j796U8XOVdeeWUWb9u2LYtrtVUbNmzI4ljvlFJZTxKfS0qd61Fr\n8zH+nbWawYNZR0i7WFv6r//6r0XObbfdlsVxPqZUzrd58+Zl8SmnnFKMiXWEsSYqpZQ+8pGPZPHo\n0aOLnG6oG/ULFAAAQCMLKAAAgEYWUAAAAI0soAAAABo5RCKVRb833HBDkRMbNdbEIrcxY8ZkcSz2\nTCml0047LYtrBaqa1dGidrBJLEy+/vrri5zYJPX000/P4jlz5hRjYpFqrQFlnLfd2iyvv6sderBp\n06YsXr9+fZEzbNiwLG4pOj9Q4nMePHhwFtca6W7dujWLR40aVeTEv6n2N/SmKSl9Q3yt4iEN3/72\nt4sxsUF0/MxOKaVPfepTWbxw4cJf+bFjnFJKTz/9dBbPnj274/3Gw1xSKhugxoOpap8dU6dOzeLa\n9w6HSBx6tevP17/+9Sz+2te+VuTE1/x973tfx/uZO3duFteusy+//HLH57dy5cosrn03iHM9fp/u\nC3PPNxoAAIBGFlAAAACNLKAAAAAaqYFK5T7OWuOvln3uQ4cOzeLYJPL4448vxsS6qNpeULUj1HTa\nU59SSn/0R3+UxXHvcUopjR07NotnzpyZxbEuJKWyJtC87V6xPiKllO6///4sXrt2bZEzYcKELD6Y\nr3esx4gNH2tNcuP7I9b+pVQ2Ia01P49771ua5HZDU8j+pvaZ3dPTk8X/+I//mMUrVqwoxsyfPz+L\nL7vssiIn1kXV3guxHnXRokVZvHTp0mJMrPuI9akplU3NN2/eXOTEmtVnnnkmiydNmlSMie9v9dl9\n05o1a4rbPv/5z2fxrl27ipw4r3/84x8XOfG7QUv9Z2y2G5ucp5TS7bffnsU33nhjkXP22Wdn8Wc+\n85ksjvO+9vzebb7hAAAANLKAAgAAaGQBBQAA0MgCCgAAoJFDJCpqRfOxKLRWJHrMMcdk8Yc//OEs\nrhVqPv/881kcD56AX2bv3r1Z/JWvfKXIuffeezveTyzGjIXztSa5scA4NjJNySES3SIWmKeU0ksv\nvZTFtcNH+pJYPBwP9EmpPCzjxRdfLHLiNbz2WXDEEfnHZssBEYrrD77adSteDx966KEs3rhxYzFm\n+PDhWXzfffcVOfFwith8N6XyPRSb5MYGpCmVTXHjAT8plX9n7f0cD1mJ83rWrFnFmNhs10FBfUNs\ngBsPV0gppW3btmVx7Xr45S9/OYvjgREp9e66FcfE55JSSv/2b/+WxbXPl3g/8ftOX2D2AwAANLKA\nAgAAaGQBBQAA0EgNVCobI55//vlFzquvvprFcR98Sikdd9xxWTx69OgsrjU6jQ3tao3y4r5n++lJ\nqazpiHUrKZWNQWt7808++eQsjo3wZs+eXYyJ9xPfQ3SPWB+RUrkfvrY/PjbsjNe7QzknYk1KSik9\n/PDDWfzKK68UObE2JDY6T6lz41zX576hVgMVm47GOVybszt27MjixYsXFznLly/P4tr3gzgv4neK\nffv2FWNis+dY/5JSSs8991wWx+t3Sin92q/9Wha/973vzeJY75RSWdfaMq9rjVW9Hw6s+Ln/xBNP\nFDlx/p100klFzsKFCzs+Vnw9Y1xrwv7oo49m8ec+97ki5/HHH8/iWi3dtGnTsnjcuHH7f7KHgF+g\nAAAAGllAAQAANLKAAgAAaGQBBQAA0MghEqksYPvSl75U5MTi0scee6zIiUWgsTnYli1bijHxEIlB\ngwYVOdddd10W1wo+6d9qxbmxoHjKlClFzimnnJLFsTAzpZTmz5+fxbGpYu0QidiYT6Fw96pdcy64\n4IIsfuqpp4qcWLwci85rRb8xp1a035ti9dgAtdZUOhY3xwbSKZXNz2sHbLQ0zuXQqx3kcOmll2Zx\nPMhh69atxZhYKF8rnF+2bFkWb9++veP97Ny5s+PzjfezevXqIifO0eOPP77ImTp1ahbHw4Vq14De\nHI7ivfDui41paweLxOtsbc7eeOONWXzWWWcVOc8++2wWx++9zz//fDFm3bp1WVx7T8Vr6HnnnVfk\n3HTTTVkcv/f2hbnmFygAAIBGFlAAAACNLKAAAAAaHfZ2rbhigKs14Hv55Zez+Hvf+16Rs2LFiiyO\njXNr+5djXVStbuBv/uZvsvijH/1okRP3vNK/1N6mcS/0D37wgyJn1apVWRybnaZUNoA++uijs7i2\np14j3f6jNrfinvm4Fz6lzk1Jd+/eXYyJdXq1Bo+jRo3K4toe/zivb7jhhiy+/fbbizFxXscagJRS\nuuKKK7I4NtZNqW/svaez2ryOt7355psdx0R79+4tbou1VPH7QkplI+dYA1Wrm4r1I7WGoxdddFEW\n12pZJk6cmMWxhrW3tYgcfLFJeO31jtfH2pztjTgnajWisZ7uxBNPLHJijeppp51W5MSawL44H/0C\nBQAA0MgCCgAAoJEFFAAAQCM1UI1iXVRtj/MDDzyQxXfccUcW33vvvcWYWMdSq2X6+Mc/nsV/+qd/\nWuTEXj21nhJ0r9rbdNeuXVkc99inlNKDDz6YxbWajlh3F3uG1PZYx1qq2t78vrhnmTax7ij2uEup\nrOHYtGlTFse6kJTKOtA9e/YUOXG//osvvljkxGvrCy+8kMULFy4sxlx77bVZ/MEPfrDI0d+MA6Gl\n/irGtdrr+P6o9fMZPnx4Ftd6OsXrs15m3Stei5cvX17kfOITn8jipUuXFjnxOlubA3EuxXrpq6++\nuhgTe61Nnz69yOkvNdN+gQIAAGhkAQUAANDIAgoAAKCRBRQAAEAjh0j0UkuR6I4dO7J40aJFxZib\nbropi9euXVvknHPOOVn8kY98pMiJjchqhaR0r9p8i8WkGzduLHKWLFmSxbV5EQs646ES8+bNK8bE\nBnq1QyQOltq/jaLoA6vlYyIePFFrgBvnbK0oPs7jeNBOLSfOxxNOOKEYM378+CzWPJRu41pH1PJd\ntHZASTxEonbwWLxGOnwk5xcoAACARhZQAAAAjSygAAAAGqmBOsTiPtRaY8m4v7/WbJeBJ9aY7N69\nu8iJ9SO1+fXmm29mcayBGjVqVDEmzsGBvheaNp2aibbmRC17881RAA4Uv0ABAAA0soACAABoZAEF\nAADQyAIKAACgkUMkoEu1NC7tNCalsrlpVDu0JDbYaynQ1wQSAOgP/AIFAADQyAIKAACgkQUUAABA\nIzVQ0KVa3rqxxqilcWmn+/hltwEADAR+gQIAAGhkAQUAANDIAgoAAKCRBRQAAEAjh0gAAAA08gsU\nAABAIwsoAACARhZQAAAAjSygAAAAGllAAQAANLKAAgAAaGQBBQAA0MgCCgAAoJEFFAAAQCMLKAAA\ngEYWUAAAAI0soAAAABpZQAEAADSygAIAAGhkAQUAANDIAgoAAKCRBRQAAEAjCygAAIBGFlAAAACN\nLKAAAAAaWUABAAA0soACAABoZAEFAADQyAIKAACgkQUUAABAIwsoAACARhZQAAAAjf4ftpwP9w2n\nqtAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1080x360 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "xUocDdHJaGJH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "23e3cfd2-ea75-451b-9233-9047a2bc893e"
      },
      "cell_type": "code",
      "source": [
        "out.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 1, 28, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "VJWnBB08aMbX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}