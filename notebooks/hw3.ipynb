{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xhxuciedu/CS284A/blob/master/hw3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS184A/284A: AI in Biology and Medicine\n",
        "# HW3 \n"
      ],
      "metadata": {
        "id": "ZH1TS3rc3API"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting TF Binding Sites\n",
        "\n",
        "Transcription Factors (TFs) are proteins that bind to the DNA and help regulate gene transcription. The TFs have to recognize some \"motif\" on the DNA upstream from the gene, and DNA accessibility also plays a role.  \n",
        "\n",
        "In tis problem set, we will develop ML methods to predict which sequences can be bound by a transcription factor called JUND.  The binding profile of JUND expressed in terms of a sequence logo is shown in the following picture.\n",
        "\n",
        "More information on JUND can be found here:\n",
        "- https://www.genecards.org/cgi-bin/carddisp.pl?gene=JUND"
      ],
      "metadata": {
        "id": "JBTsc7NV6hih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the image\n",
        "from IPython import display\n",
        "display.Image(\"https://www.ismara.unibas.ch/ISMARA/scratch/NHBE_SC2/ismara_report/logos/JUND.png\", width=350)"
      ],
      "metadata": {
        "id": "03Kb0PxS7kmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## MLP model \n",
        "\n",
        "In this assignment you'll write an MLP model the predict whether a segments of the human chromosome 22 (Chr22) contain the binding sites for the JUND TF. You can modify the mlp notebook I shared with you to work on this problem. You need to have at least one hidden layer. You have to compute a weighted loss, and include accessibility information in your model, as described below.\n"
      ],
      "metadata": {
        "id": "MLaHSeDE6egJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Dataset\n",
        "The data comprises 101 length segments from Chr22, with each position a one-hot vector denoting one of the four bases (A, C, G, T). Thus, each element of the input is 2d with size 101×4. Each such element has a target label 0 or 1, indicating whether the TF binds to that segment or not. The data also includes a weight per input element, since there are only a few binding sites (0.42%), so that you'd obtain an accuracy of 99.58% just by predicting there are no binding sites. This means you have to use the weights to discount the losses for label 0 and enhance the losses for label 1 items. Finally, there is an array of values, one per input element, that also indicates the chromosome accessibility for that segment.\n",
        "\n",
        "\n",
        "Data Credit: Mohammed Zaki"
      ],
      "metadata": {
        "id": "hD6tuYsy62tq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download data\n",
        "\n",
        "\n",
        "\n",
        "*   First, you need to download data file named \"TF_data.zip\" from Canvas. Unzip it and create the train, valid, test directories.\n",
        "\n",
        "*   If you use Google Colab, you can first upload the TF_data.zip file, and then run the follow command\n",
        "\n",
        "<center><code>!unzip TF_data.zip</code></center>\n",
        "\n",
        "*   The data is split into training, validation and testing sets. Each set contains the following files:\n",
        "  * shard-0-X.joblib: the set of 101 x 4 input elements\n",
        "  * shard-0-y.joblib: the true labels: 0 or 1\n",
        "  * shard-0-w.joblib: weight per input element\n",
        "  * shard-0-a.joblib: accessibility value per input element\n",
        "\n",
        "*   After unzip the data file, you  can read these files by using joblib.load function, which will populate a numpy array. For example\n",
        "<center><code>X = joblib.load('shard-0-X.joblib')</code></center>\n",
        "will results in a numpy array X, which you can then convert to torch tensor, and so on.\n",
        "\n",
        "*  The roles of training, validation and testing sets:\n",
        "  * Use training set to tune the parameters of the model. \n",
        "  * Use validation set to select model structure and hyperparamters (e.g., number of epochs, learning rate, etc). \n",
        "  * Use test set for the final evaluation.  You should never touch test set for either your model training or model selection. "
      ],
      "metadata": {
        "id": "Ov55aatI4sDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment the following command if you run this code in Google Colab\n",
        "#!unzip TF_data.zip"
      ],
      "metadata": {
        "id": "t7pHzBVL491Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lODww8KHp-Y8"
      },
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "from sklearn.metrics import precision_score, recall_score, confusion_matrix, f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up DataLoader for fetching training and testing data\n",
        "\n",
        "Because we use mini-batch gradient descent for training.  We need to set up dataloader that can provide us a minibatch of data samples. \n",
        "\n"
      ],
      "metadata": {
        "id": "asnHxt3K4qmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch._C import dtype\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class JUND_Dataset(Dataset):\n",
        "    def __init__(self, data_dir):\n",
        "        '''load X, y, w, a from data_dir'''        \n",
        "        super(JUND_Dataset, self).__init__()\n",
        "\n",
        "        # load X, y, w, a from given data_dir\n",
        "        # convert them into torch tensors\n",
        "        self.X = torch.from_numpy(joblib.load(data_dir + '/shard-0-X.joblib')).float()\n",
        "        self.y = torch.from_numpy(joblib.load(data_dir + '/shard-0-y.joblib')).float()\n",
        "        self.w = torch.from_numpy(joblib.load(data_dir + '/shard-0-w.joblib')).float()\n",
        "        self.a = torch.from_numpy(joblib.load(data_dir + '/shard-0-a.joblib')).float()\n",
        "\n",
        "    def __len__(self):\n",
        "        '''return len of dataset'''\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        '''return X, y, w, and a values at index idx'''\n",
        "        return self.X[idx],self.y[idx],self.w[idx], self.a[idx]"
      ],
      "metadata": {
        "id": "vkSjbqBUyoE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get data\n",
        "# \n",
        "# You may need to change the directory if the data are not stored under current directory\n",
        "# \n",
        "train_dataset = JUND_Dataset('train_dataset')\n",
        "test_dataset = JUND_Dataset('test_dataset')"
      ],
      "metadata": {
        "id": "_l9lBPtb327G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S2r6B6sAuA_g"
      },
      "cell_type": "markdown",
      "source": [
        "### Traing and test data"
      ]
    },
    {
      "metadata": {
        "id": "WQOLl50SqWNI"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 100   \n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "va0gwf0MgK_4"
      },
      "cell_type": "code",
      "source": [
        "print('Train data: ', len(train_dataset))\n",
        "print('Test data: ', len(test_dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your result should look like:\n",
        "\n",
        "```\n",
        "Train data:  276216\n",
        "Test data:  34528\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "73bkK9eDKRkO"
      }
    },
    {
      "metadata": {
        "id": "64kspZdouNQ_"
      },
      "cell_type": "markdown",
      "source": [
        "### Fetch a minibatch and check the size of the data"
      ]
    },
    {
      "metadata": {
        "id": "MMGdSgKUqtT7"
      },
      "cell_type": "code",
      "source": [
        "X,y,w,a = next(iter(train_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the following code to check the size of data in each minibatch\n",
        "X.shape, y.shape, w.shape, a.shape"
      ],
      "metadata": {
        "id": "QTmB38Jn6TBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your result should look like the following:\n",
        "\n",
        "```\n",
        "(torch.Size([100, 101, 4]),\n",
        " torch.Size([100, 1]),\n",
        " torch.Size([100, 1]),\n",
        " torch.Size([100, 1]))\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "HBvOgZ7TGnui"
      }
    },
    {
      "metadata": {
        "id": "9eMh1Yuetq3c"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem 1 - MLP\n",
        "\n",
        "Define an MLP (multi-layer perceptron) with at least one hidden layer to predict the labels given inputs. \n",
        "\n",
        "Please note the following:\n",
        "\n",
        "- The label for each input is either 0 or 1, so this is essentially a binary classification problem. \n",
        "- Input consist of both X and a:\n",
        "  - X: represents the DNA sequence. Each position is a one-hot vector denoting one of the four bases (A, C, G, T). Thus, each element of the input is 2d with size 101×4.   Since each input is treated as a vector in MLP, the 2d array needs to be flattened into a 404-dimensional vector. \n",
        "  - a: represents the chromatin accessiblity of the input DNA sequence segment. You can think of \"a\" as an additional feature for each input. You can decide how to use it.  For instance, if you are using hidden dimension of 128, then after concatenating the accessibility value, it will become a 129d vector, which should be fed to the final output layer of size 1, since we have a binary class/label.\n",
        "\n",
        "An initial template code, representing a simple model, is provided.  Your job is to change the definition of the model to improve the model's performance on the test dataset. \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "#  modify this code block\n",
        "###############################################################################\n",
        "# MLP\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, input_size=101*4, hidden_size=128):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU())\n",
        "        self.fc = nn.Linear(hidden_size+1, 1)\n",
        "        \n",
        "    def forward(self, X, a):\n",
        "        out = X.reshape(X.size(0),-1)\n",
        "        out = self.layer1(out)\n",
        "        out = torch.cat((out, a),1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "# end of model definition\n",
        "###############################################################################"
      ],
      "metadata": {
        "id": "n3nw65YSoomJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Have a test run of your model"
      ],
      "metadata": {
        "id": "AjI53mTjLYwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel()\n",
        "output = model(X,a)"
      ],
      "metadata": {
        "id": "BoZaA5AgqYz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your model should run smoothly.  The size of output should be 100 - the same as the minibatch size. "
      ],
      "metadata": {
        "id": "w1BvJMMoLhrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training \n",
        "\n",
        "Next you need to define a loss function and then run gradient descend to learn the weights of the neural net. \n",
        "\n",
        "- There is a strong class imbalance problem in the training set (many more 0's than 1's). \n",
        "\n",
        "- To handle the class imbalance problem, we treat each sample differently.  Each data point is assigned a weight. Take a look at the variable named \"w\" in each data set.  \n",
        "\n",
        "- Define a loss function, in which the total loss is a weighted combination of losses coming from each sample.  Use the weights specified in \"w\".   Note that this definition is different from our typical loss, where each sample contributes equally to the final total loss.\n",
        "\n",
        "- You should use binary_cross_entropy_with_logits with weight set to the weights per input element. Check out the documentation for details."
      ],
      "metadata": {
        "id": "UgxCUABnLtpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################\n",
        "# complete this code block\n",
        "# define loss and optimizer\n",
        "###############################################################\n",
        "criterion = \n",
        "###############################################################"
      ],
      "metadata": {
        "id": "3UIedUtTr8-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# have a test run to check of your loss is defined properly\n",
        "loss = criterion(output, y)"
      ],
      "metadata": {
        "id": "dZV2RYPMtbIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "4oj_vfq-r9BX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## You task:\n",
        "\n",
        "You need to train the model on the training data, and use the **validation data** to select how many epochs you want to use and to choose the hidden dimension. \n",
        "\n",
        "Use the **weighted prediction accuracy** as the evaluation metric. That is, sum of the weights of the correct predictions divided by the total weight across all the input elements. \n",
        "\n",
        "Finally, report the **weighted accuracy on the test data.**"
      ],
      "metadata": {
        "id": "tiR7vUEvtbFi"
      }
    },
    {
      "metadata": {
        "id": "JbElIMf1tfzK"
      },
      "cell_type": "markdown",
      "source": [
        "### Run Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose hyper parameters and optimizer \n",
        "num_epochs = 5\n",
        "learning_rate = 0.001\n",
        "optimizer = "
      ],
      "metadata": {
        "id": "hDBh5Tok9EX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "etenh_AUqQDC"
      },
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (X, y, w, a) in enumerate(train_loader):\n",
        "        # images = images.to(device)\n",
        "        # labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        output = model(X, a)\n",
        "        criterion.weight = w\n",
        "        loss = criterion(output, y)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BXL4HBkItiG8"
      },
      "cell_type": "markdown",
      "source": [
        "## Final Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate predictions on test set"
      ],
      "metadata": {
        "id": "JyBpxxu9RlHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "y_pred_list = []\n",
        "y_target_list = []\n",
        "model.eval()\n",
        "#Since we don't need model to back propagate the gradients in test set we use torch.no_grad()\n",
        "# reduces memory usage and speeds up computation\n",
        "with torch.no_grad():\n",
        "     for i, (X, y, w, a) in enumerate(test_loader):\n",
        "        output = model(X, a)\n",
        "        y_pred_tag = (output>0).int()\n",
        "        y_pred_list.append(y_pred_tag.detach().numpy())\n",
        "        y_target_list.append(y.detach().numpy())\n",
        "\n",
        "#Takes arrays and makes them list of list for each batch        \n",
        "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
        "#flattens the lists in sequence\n",
        "ytest_pred = list(itertools.chain.from_iterable(y_pred_list))\n",
        "\n",
        "\n",
        "#Takes arrays and makes them list of list for each batch        \n",
        "y_target_list = [a.squeeze().tolist() for a in y_target_list]\n",
        "#flattens the lists in sequence\n",
        "ytest_target = list(itertools.chain.from_iterable(y_target_list))"
      ],
      "metadata": {
        "id": "GShgVdMXwAck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Report\n",
        "- Precision\n",
        "- Recall\n",
        "- F1 Score\n",
        "- Confusion Matrix"
      ],
      "metadata": {
        "id": "73HRUUviwgvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conf_matrix = confusion_matrix(ytest_target ,ytest_pred)\n",
        "print(\"Confusion Matrix of the Test Set\")\n",
        "print(\"-----------\")\n",
        "print(conf_matrix)\n",
        "print(\"Precision of the MLP :\\t\"+str(precision_score(ytest_target,ytest_pred)))\n",
        "print(\"Recall of the MLP    :\\t\"+str(recall_score(ytest_target,ytest_pred)))\n",
        "print(\"F1 Score of the Model :\\t\"+str(f1_score(ytest_target,ytest_pred)))"
      ],
      "metadata": {
        "id": "lOeSBAMgwWlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Write code to **weighted prediction accuracy** \n",
        "\n",
        "That is, sum of the weights of the correct predictions divided by the total weight across all the input elements.\n",
        "\n",
        "Report the weighted accuracy on the test data.\n",
        "\n"
      ],
      "metadata": {
        "id": "2aSESJJ4R27U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###########################################################\n",
        "#\n",
        "#  Complete the following function which calculates weight prediction accuracy\n",
        "#\n",
        "def weight_accuracy(predicted_y, true_y, weight):\n",
        "  '''\n",
        "    Inputs:  \n",
        "         predicted_y:  predicted labels \n",
        "         true_y:  true labels\n",
        "         weight:  weight of each sample\n",
        "    return:  \n",
        "        sum of the weights of the correct predictions divided by the total weight across all the input elements\n",
        "  '''\n",
        "\n",
        "\n",
        "###########################################################"
      ],
      "metadata": {
        "id": "8hJstR59SM6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate weighted accuracy on the test data"
      ],
      "metadata": {
        "id": "-LFgWrpBwaFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2 - CNN\n",
        "\n",
        "- Define a CNN model to solve the above problem. \n",
        "- The CNN model receives the same inputs. Instead of using MLP, it uses convolution to extract features.  *italicized text*\n",
        "- Different from 2d images, the convolution will be 1d convolution\n",
        "- Again, use validation set to design your model and select hyperparamters. \n",
        "- Report weight accuracy on test set\n",
        "- Comment on your observations. "
      ],
      "metadata": {
        "id": "gc10LliyS9NO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN Details\n",
        "- For the CNN model, you have to use the 1D convolution module. The in_channels will be 4, one per DNA base. You can decide what number of out_channels and kernel size you want to use. \n",
        "\n",
        "- Check out torch Conv1d\n",
        "\n",
        "```\n",
        "torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0) \n",
        "```\n",
        "\n",
        "\n",
        "- Note the dimensions of the input required for the 1D convolution -- (N,C,L) a where N is a batch size, C denotes a number of channels, L is a length of signal sequence.\n",
        "\n",
        "- That is the input sequence has to be 4×101 and not 101×4 as in the data. So you should use torch.swapaxes function to swap the last two axes (not the batch axis) in the forward function.\n",
        "\n",
        "After the Conv1d, you can apply a relu activation, do dropout, and then try a maxpooling layer (1d). You can try more than one convolution layer too.\n",
        "\n",
        "Finally, flatten out the last convolution layer and use as input to an MLP. "
      ],
      "metadata": {
        "id": "YLs6BPOrwwUn"
      }
    },
    {
      "metadata": {
        "id": "c66Qy3nMqT-1"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 3 - LSTM\n",
        "\n",
        "\n",
        "- For the LSTM model, use the encoder followed by a two layer MLP approach. That is, pass the input sequence (batch) through the LSTM and use the last hidden layer as the representation or embedding vector for the sequence. You can choose the dimensionality of the hidden layer. Next, use this vector as input to a two fully connected MLP layers -- the first connects the input vector to the hidden layer (again you can choose the size of the hidden layer), and the second connects the hidden to the output neuron. Use dropout and relu as appropriate.\n",
        "\n",
        "- Keep in mind that for the input to the LSTM module in pytorch use batch_first=True. This means that the batch dimension comes first, so the input is (N×101×4), which is how the input data is structured. Make note of the output of the LSTM layer so that you store the last hidden layer as the representation, to be used as input to the MLP layers.\n",
        "\n",
        "- Also, before feeding the output of the hidden layer to the output layer, you must concatenate the accessibility value. So if you are using hidden dimension of 128, then after concatenating the accessibility value, it will become a 129d vector, which should be fed to the final output layer of size 1, since we have a binary class/label.\n",
        "\n",
        "- You should use binary_cross_entropy_with_logits with weight set to the weights per input element.\n",
        "\n",
        "- You need to train the model on the training data, and use the validation data to select how many epochs you want to use and to choose the hidden dimension. Use the weighted prediction accuracy as the evaluation metric. That is, sum of the weights of the correct predictions divided by the total weight across all the input elements. Finally, report the weighted accuracy on the test data.\n"
      ],
      "metadata": {
        "id": "JeuIdpWVzH2J"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Per7hvXrxhNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statement of Collaboration \n",
        "\n",
        "It is mandatory to include a Statement of Collaboration in each submission, with respect to the guidelines below.\n",
        "Include the names of everyone involved in the discussions (especially in-person ones), and what was discussed.\n",
        "\n",
        "All students are required to follow the academic honesty guidelines posted on the course website.\n",
        "For programming assignments, in particular, I encourage the students to organize (perhaps using Campuswire) to discuss the task descriptions, requirements, bugs in my code, and the relevant technical content before they start working on it.\n",
        "\n",
        "However, you should not discuss the specific solutions, and, as a guiding principle, you are not allowed to take anything written or drawn away from these discussions (i.e. no photographs of the blackboard, written notes, referring to Campuswire, etc.).\n",
        "\n",
        "Especially after you have started working on the assignment, try to restrict the discussion to Campuswire as much as possible, so that there is no doubt as to the extent of your collaboration.\n"
      ],
      "metadata": {
        "id": "O7CjUDjnv0t-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Complete your statement of collaboration here:\n",
        "\n"
      ],
      "metadata": {
        "id": "IyDst7etv4lx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FeGWwIStwUAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What to submit\n",
        "- Export a notebook as PDF\n",
        "  - Go to Main menu | File and select Print . pdf.\n",
        "\n",
        "- Upload your jupyter notebook PDF on gradescope\n",
        "\n",
        "- The notebook must have output values for the final test accuracy.\n",
        "\n",
        "- Do not submit the data file or directories."
      ],
      "metadata": {
        "id": "Tw4l4Rtouyqx"
      }
    }
  ]
}