{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer Models: A Deep Dive Tutorial\n",
        "\n",
        "**CS284A - Deep Learning**\n",
        "\n",
        "This tutorial provides a comprehensive introduction to Transformer models, implementing a complete transformer from scratch using PyTorch. We'll cover:\n",
        "\n",
        "1. **Attention Mechanisms**: Understanding self-attention and multi-head attention\n",
        "2. **Transformer Architecture**: Encoder-decoder structure with positional encoding\n",
        "3. **Implementation**: Building all components from scratch\n",
        "4. **Training**: Training on a sequence-to-sequence task\n",
        "5. **Visualization**: Visualizing attention patterns and training progress\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this tutorial, you will:\n",
        "- Understand the core components of transformer models\n",
        "- Be able to implement attention mechanisms from scratch\n",
        "- Know how to build and train transformer models\n",
        "- Visualize attention patterns to understand model behavior\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Set style for better visualizations\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8')\n",
        "except:\n",
        "    try:\n",
        "        plt.style.use('seaborn')\n",
        "    except:\n",
        "        pass\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Understanding the Transformer Architecture\n",
        "\n",
        "The Transformer model, introduced in \"Attention Is All You Need\" (Vaswani et al., 2017), revolutionized deep learning by replacing recurrent and convolutional layers with attention mechanisms.\n",
        "\n",
        "### Key Components:\n",
        "1. **Self-Attention**: Allows each position to attend to all positions in the sequence\n",
        "2. **Multi-Head Attention**: Multiple attention heads capture different types of relationships\n",
        "3. **Positional Encoding**: Injects information about token positions\n",
        "4. **Feed-Forward Networks**: Point-wise fully connected layers\n",
        "5. **Layer Normalization & Residual Connections**: Enable deep networks to train effectively\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Implementing Core Components\n",
        "\n",
        "Let's start by implementing the fundamental building blocks of a transformer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Positional Encoding\n",
        "\n",
        "Since transformers don't have recurrence or convolution, we need to add positional information to help the model understand the order of tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements positional encoding using sine and cosine functions.\n",
        "    \n",
        "    PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
        "    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Create a matrix of shape (max_len, d_model)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        \n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # Shape: (1, max_len, d_model)\n",
        "        \n",
        "        # Register as buffer (not a parameter, but part of the model state)\n",
        "        self.register_buffer('pe', pe)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor of shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "# Visualize positional encoding\n",
        "d_model = 128\n",
        "max_len = 100\n",
        "pos_encoder = PositionalEncoding(d_model, max_len, dropout=0.0)  # No dropout for visualization\n",
        "\n",
        "# Create a dummy input\n",
        "dummy_input = torch.zeros(1, max_len, d_model)\n",
        "pos_encoded = pos_encoder(dummy_input)\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(pos_encoded[0].T, aspect='auto', cmap='RdYlBu')\n",
        "plt.colorbar(label='Value')\n",
        "plt.xlabel('Position')\n",
        "plt.ylabel('Dimension')\n",
        "plt.title('Positional Encoding Visualization')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Multi-Head Self-Attention\n",
        "\n",
        "This is the core innovation of transformers. It allows the model to jointly attend to information from different representation subspaces at different positions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Head Self-Attention mechanism.\n",
        "    \n",
        "    Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        \n",
        "        # Linear projections for Q, K, V\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = math.sqrt(self.d_k)\n",
        "    \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            query: (batch_size, seq_len, d_model)\n",
        "            key: (batch_size, seq_len, d_model)\n",
        "            value: (batch_size, seq_len, d_model)\n",
        "            mask: (batch_size, seq_len, seq_len) - optional attention mask\n",
        "        Returns:\n",
        "            output: (batch_size, seq_len, d_model)\n",
        "            attention_weights: (batch_size, num_heads, seq_len, seq_len) - for visualization\n",
        "        \"\"\"\n",
        "        batch_size = query.size(0)\n",
        "        seq_len = query.size(1)\n",
        "        \n",
        "        # Linear projections and reshape for multi-head attention\n",
        "        Q = self.W_q(query).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_k(key).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(value).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        \n",
        "        # Scaled dot-product attention\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
        "        \n",
        "        # Apply mask if provided (set masked positions to -inf)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        \n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "        \n",
        "        # Apply attention to values\n",
        "        context = torch.matmul(attention_weights, V)\n",
        "        \n",
        "        # Concatenate heads\n",
        "        context = context.transpose(1, 2).contiguous().view(\n",
        "            batch_size, seq_len, self.d_model\n",
        "        )\n",
        "        \n",
        "        # Final linear projection\n",
        "        output = self.W_o(context)\n",
        "        \n",
        "        return output, attention_weights\n",
        "\n",
        "# Test the attention mechanism\n",
        "d_model = 128\n",
        "num_heads = 8\n",
        "batch_size = 2\n",
        "seq_len = 10\n",
        "\n",
        "attention = MultiHeadAttention(d_model, num_heads)\n",
        "x = torch.randn(batch_size, seq_len, d_model)\n",
        "output, attn_weights = attention(x, x, x)\n",
        "\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Attention weights shape: {attn_weights.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Position-wise Feed-Forward Network.\n",
        "    FFN(x) = max(0, xW1 + b1)W2 + b2\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Encoder Layer\n",
        "\n",
        "Combines multi-head attention, feed-forward network, layer normalization, and residual connections.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer Encoder Layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        # Self-attention with residual connection and layer norm\n",
        "        attn_output, attn_weights = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "        \n",
        "        # Feed-forward with residual connection and layer norm\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout2(ff_output))\n",
        "        \n",
        "        return x, attn_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5 Decoder Layer\n",
        "\n",
        "Similar to encoder but with an additional cross-attention mechanism to attend to encoder outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer Decoder Layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
        "        # Self-attention (with causal mask for autoregressive generation)\n",
        "        attn_output, self_attn_weights = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "        \n",
        "        # Cross-attention (attending to encoder output)\n",
        "        cross_attn_output, cross_attn_weights = self.cross_attn(\n",
        "            x, encoder_output, encoder_output, src_mask\n",
        "        )\n",
        "        x = self.norm2(x + self.dropout2(cross_attn_output))\n",
        "        \n",
        "        # Feed-forward\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout3(ff_output))\n",
        "        \n",
        "        return x, self_attn_weights, cross_attn_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.6 Complete Transformer Model\n",
        "\n",
        "Now let's put it all together to create a full transformer model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete Transformer model (Encoder-Decoder architecture).\n",
        "    \"\"\"\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8, \n",
        "                 num_encoder_layers=6, num_decoder_layers=6, d_ff=2048, \n",
        "                 max_seq_length=5000, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        \n",
        "        # Embeddings\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        \n",
        "        # Positional encodings\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_seq_length, dropout)\n",
        "        self.pos_decoder = PositionalEncoding(d_model, max_seq_length, dropout)\n",
        "        \n",
        "        # Encoder and Decoder stacks\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, num_heads, d_ff, dropout) \n",
        "            for _ in range(num_encoder_layers)\n",
        "        ])\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, num_heads, d_ff, dropout) \n",
        "            for _ in range(num_decoder_layers)\n",
        "        ])\n",
        "        \n",
        "        # Output projection\n",
        "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.d_model = d_model\n",
        "    \n",
        "    def generate_mask(self, src, tgt):\n",
        "        \"\"\"\n",
        "        Generate masks for source and target sequences.\n",
        "        \"\"\"\n",
        "        batch_size = src.size(0)\n",
        "        src_len = src.size(1)\n",
        "        tgt_len = tgt.size(1)\n",
        "        \n",
        "        # Source mask: (batch_size, 1, 1, src_len)\n",
        "        # True where we can attend, False for padding\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        \n",
        "        # Target mask: combine padding mask and causal mask\n",
        "        # Padding mask: (batch_size, 1, tgt_len, 1) - True for valid tokens\n",
        "        tgt_padding_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
        "        \n",
        "        # Causal mask: (1, 1, tgt_len, tgt_len) - True where j <= i\n",
        "        nopeak_mask = torch.triu(torch.ones(tgt_len, tgt_len), diagonal=1).bool()\n",
        "        nopeak_mask = nopeak_mask.unsqueeze(0).unsqueeze(0).to(tgt.device)\n",
        "        \n",
        "        # Combine: can attend if not padding AND not future position\n",
        "        # (batch_size, 1, tgt_len, tgt_len)\n",
        "        tgt_mask = tgt_padding_mask.expand(batch_size, 1, tgt_len, tgt_len) & (~nopeak_mask)\n",
        "        \n",
        "        return src_mask, tgt_mask\n",
        "    \n",
        "    def forward(self, src, tgt):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Source sequence (batch_size, src_len)\n",
        "            tgt: Target sequence (batch_size, tgt_len)\n",
        "        Returns:\n",
        "            output: (batch_size, tgt_len, tgt_vocab_size)\n",
        "        \"\"\"\n",
        "        # Generate masks\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "        \n",
        "        # Encoder\n",
        "        src_embedded = self.dropout(self.pos_encoder(\n",
        "            self.encoder_embedding(src) * math.sqrt(self.d_model)\n",
        "        ))\n",
        "        \n",
        "        encoder_output = src_embedded\n",
        "        encoder_attentions = []\n",
        "        for encoder_layer in self.encoder_layers:\n",
        "            encoder_output, attn_weights = encoder_layer(encoder_output, src_mask)\n",
        "            encoder_attentions.append(attn_weights)\n",
        "        \n",
        "        # Decoder\n",
        "        tgt_embedded = self.dropout(self.pos_decoder(\n",
        "            self.decoder_embedding(tgt) * math.sqrt(self.d_model)\n",
        "        ))\n",
        "        \n",
        "        decoder_output = tgt_embedded\n",
        "        decoder_self_attentions = []\n",
        "        decoder_cross_attentions = []\n",
        "        for decoder_layer in self.decoder_layers:\n",
        "            decoder_output, self_attn, cross_attn = decoder_layer(\n",
        "                decoder_output, encoder_output, src_mask, tgt_mask\n",
        "            )\n",
        "            decoder_self_attentions.append(self_attn)\n",
        "            decoder_cross_attentions.append(cross_attn)\n",
        "        \n",
        "        # Output projection\n",
        "        output = self.fc_out(decoder_output)\n",
        "        \n",
        "        return output, encoder_attentions, decoder_self_attentions, decoder_cross_attentions\n",
        "\n",
        "print(\"Transformer model defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SequenceCopyDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for sequence copying with reversal task.\n",
        "    Input: [1, 2, 3, 4, 5]\n",
        "    Output: [1, 2, 3, 4, 5, 5, 4, 3, 2, 1] (copy + reverse)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_samples=10000, min_len=3, max_len=10, vocab_size=20):\n",
        "        self.num_samples = num_samples\n",
        "        self.min_len = min_len\n",
        "        self.max_len = max_len\n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "        # Special tokens: 0 = PAD, 1 = SOS (Start of Sequence), 2 = EOS (End of Sequence)\n",
        "        self.PAD = 0\n",
        "        self.SOS = 1\n",
        "        self.EOS = 2\n",
        "        \n",
        "        self.data = []\n",
        "        self._generate_data()\n",
        "    \n",
        "    def _generate_data(self):\n",
        "        \"\"\"Generate random sequences and their targets.\"\"\"\n",
        "        for _ in range(self.num_samples):\n",
        "            # Random sequence length\n",
        "            seq_len = random.randint(self.min_len, self.max_len)\n",
        "            \n",
        "            # Generate random sequence (starting from token 3 to avoid special tokens)\n",
        "            input_seq = [random.randint(3, self.vocab_size - 1) for _ in range(seq_len)]\n",
        "            \n",
        "            # Target: copy + reverse\n",
        "            target_seq = input_seq + input_seq[::-1]\n",
        "            \n",
        "            # Add SOS and EOS tokens\n",
        "            input_seq = [self.SOS] + input_seq + [self.EOS]\n",
        "            target_seq = [self.SOS] + target_seq + [self.EOS]\n",
        "            \n",
        "            self.data.append((input_seq, target_seq))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        input_seq, target_seq = self.data[idx]\n",
        "        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)\n",
        "\n",
        "# Create dataset\n",
        "train_dataset = SequenceCopyDataset(num_samples=5000, min_len=3, max_len=10, vocab_size=20)\n",
        "test_dataset = SequenceCopyDataset(num_samples=1000, min_len=3, max_len=10, vocab_size=20)\n",
        "\n",
        "# Show some examples\n",
        "print(\"Sample examples:\")\n",
        "for i in range(3):\n",
        "    src, tgt = train_dataset[i]\n",
        "    print(f\"Input:  {src.tolist()}\")\n",
        "    print(f\"Target: {tgt.tolist()}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training Setup\n",
        "\n",
        "Let's set up the training loop with proper data loading, loss function, and optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "VOCAB_SIZE = 20  # Including special tokens (PAD=0, SOS=1, EOS=2)\n",
        "D_MODEL = 128\n",
        "NUM_HEADS = 8\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "D_FF = 512\n",
        "MAX_SEQ_LENGTH = 50\n",
        "DROPOUT = 0.1\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 20\n",
        "LEARNING_RATE = 0.0001\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Initialize model\n",
        "model = Transformer(\n",
        "    src_vocab_size=VOCAB_SIZE,\n",
        "    tgt_vocab_size=VOCAB_SIZE,\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    num_encoder_layers=NUM_ENCODER_LAYERS,\n",
        "    num_decoder_layers=NUM_DECODER_LAYERS,\n",
        "    d_ff=D_FF,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dropout=DROPOUT\n",
        ").to(device)\n",
        "\n",
        "# Loss function (ignoring padding tokens)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training Loop\n",
        "\n",
        "Now let's train the model and track the loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for src, tgt in tqdm(loader, desc=\"Training\"):\n",
        "        src = src.to(device)\n",
        "        tgt = tgt.to(device)\n",
        "        \n",
        "        # Prepare input and target for decoder\n",
        "        # Input to decoder: tgt[:-1], Target: tgt[1:]\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        tgt_output = tgt[:, 1:]\n",
        "        \n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        output, _, _, _ = model(src, tgt_input)\n",
        "        \n",
        "        # Reshape for loss calculation\n",
        "        output = output.reshape(-1, output.shape[-1])\n",
        "        tgt_output = tgt_output.reshape(-1)\n",
        "        \n",
        "        # Calculate loss\n",
        "        loss = criterion(output, tgt_output)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# Training\n",
        "train_losses = []\n",
        "print(\"Starting training...\\n\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    epoch_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    scheduler.step()\n",
        "    train_losses.append(epoch_loss)\n",
        "    \n",
        "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {epoch_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "print(\"\\nTraining completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Visualize Training Loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses, linewidth=2)\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Training Loss', fontsize=12)\n",
        "plt.title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Testing and Evaluation\n",
        "\n",
        "Let's evaluate the model on test data and see how well it performs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, loader, criterion, device):\n",
        "    \"\"\"Evaluate the model on test data.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for src, tgt in tqdm(loader, desc=\"Evaluating\"):\n",
        "            src = src.to(device)\n",
        "            tgt = tgt.to(device)\n",
        "            \n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "            \n",
        "            output, _, _, _ = model(src, tgt_input)\n",
        "            output = output.reshape(-1, output.shape[-1])\n",
        "            tgt_output = tgt_output.reshape(-1)\n",
        "            \n",
        "            loss = criterion(output, tgt_output)\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            # Calculate accuracy (excluding padding)\n",
        "            predictions = output.argmax(dim=-1)\n",
        "            mask = (tgt_output != 0)\n",
        "            correct += (predictions[mask] == tgt_output[mask]).sum().item()\n",
        "            total += mask.sum().item()\n",
        "    \n",
        "    avg_loss = total_loss / len(loader)\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Evaluate\n",
        "test_loss, test_accuracy = evaluate(model, test_loader, criterion, device)\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Generate Predictions\n",
        "\n",
        "Let's see the model generate sequences on some test examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_sequence(model, src, max_len=50, device='cpu'):\n",
        "    \"\"\"\n",
        "    Generate output sequence given input sequence.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    src = src.unsqueeze(0).to(device) if src.dim() == 1 else src.to(device)\n",
        "    \n",
        "    # Start with SOS token\n",
        "    tgt = torch.tensor([[1]], dtype=torch.long).to(device)  # SOS = 1\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len):\n",
        "            output, _, _, _ = model(src, tgt)\n",
        "            next_token = output[:, -1, :].argmax(dim=-1, keepdim=True)\n",
        "            tgt = torch.cat([tgt, next_token], dim=1)\n",
        "            \n",
        "            # Stop if EOS token (2) is generated\n",
        "            if next_token.item() == 2:\n",
        "                break\n",
        "    \n",
        "    return tgt[0]\n",
        "\n",
        "# Test on some examples\n",
        "model.eval()\n",
        "print(\"Sample Predictions:\\n\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for i in range(5):\n",
        "    src, tgt_true = test_dataset[i]\n",
        "    tgt_pred = generate_sequence(model, src, max_len=50, device=device)\n",
        "    \n",
        "    # Remove SOS and EOS tokens for display\n",
        "    src_display = [x for x in src.tolist() if x not in [0, 1, 2]]\n",
        "    tgt_true_display = [x for x in tgt_true.tolist() if x not in [0, 1, 2]]\n",
        "    tgt_pred_display = [x for x in tgt_pred.tolist() if x not in [0, 1, 2]]\n",
        "    \n",
        "    print(f\"Example {i+1}:\")\n",
        "    print(f\"  Input:  {src_display}\")\n",
        "    print(f\"  Target: {tgt_true_display}\")\n",
        "    print(f\"  Pred:   {tgt_pred_display}\")\n",
        "    print(f\"  Match:  {'✓' if tgt_pred_display == tgt_true_display else '✗'}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualizing Attention Patterns\n",
        "\n",
        "One of the most powerful features of transformers is their interpretability through attention visualization. Let's visualize what the model is \"paying attention to\" when making predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_attention(model, src, tgt_input, layer_idx=0, head_idx=0, device='cpu'):\n",
        "    \"\"\"\n",
        "    Visualize attention weights from a specific layer and head.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    src = src.unsqueeze(0).to(device) if src.dim() == 1 else src.to(device)\n",
        "    tgt_input = tgt_input.unsqueeze(0).to(device) if tgt_input.dim() == 1 else tgt_input.to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Get encoder output and attention\n",
        "        src_embedded = model.pos_encoder(\n",
        "            model.encoder_embedding(src) * math.sqrt(model.d_model)\n",
        "        )\n",
        "        encoder_output = src_embedded\n",
        "        encoder_attentions = []\n",
        "        for encoder_layer in model.encoder_layers:\n",
        "            encoder_output, attn_weights = encoder_layer(encoder_output, None)\n",
        "            encoder_attentions.append(attn_weights)\n",
        "        \n",
        "        # Get decoder attention\n",
        "        tgt_embedded = model.pos_decoder(\n",
        "            model.decoder_embedding(tgt_input) * math.sqrt(model.d_model)\n",
        "        )\n",
        "        decoder_output = tgt_embedded\n",
        "        decoder_self_attentions = []\n",
        "        decoder_cross_attentions = []\n",
        "        for decoder_layer in model.decoder_layers:\n",
        "            decoder_output, self_attn, cross_attn = decoder_layer(\n",
        "                decoder_output, encoder_output, None, None\n",
        "            )\n",
        "            decoder_self_attentions.append(self_attn)\n",
        "            decoder_cross_attentions.append(cross_attn)\n",
        "    \n",
        "    # Extract attention weights (average across batch if needed)\n",
        "    encoder_attn = encoder_attentions[layer_idx][0, head_idx].cpu().numpy()\n",
        "    decoder_self_attn = decoder_self_attentions[layer_idx][0, head_idx].cpu().numpy()\n",
        "    decoder_cross_attn = decoder_cross_attentions[layer_idx][0, head_idx].cpu().numpy()\n",
        "    \n",
        "    return encoder_attn, decoder_self_attn, decoder_cross_attn\n",
        "\n",
        "# Get a test example\n",
        "src, tgt = test_dataset[0]\n",
        "tgt_input = tgt[:-1]  # Remove last token for input\n",
        "\n",
        "# Visualize attention\n",
        "encoder_attn, decoder_self_attn, decoder_cross_attn = visualize_attention(\n",
        "    model, src, tgt_input, layer_idx=0, head_idx=0, device=device\n",
        ")\n",
        "\n",
        "# Create visualization\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Encoder self-attention\n",
        "sns.heatmap(encoder_attn, ax=axes[0], cmap='Blues', cbar=True, \n",
        "            xticklabels=src.tolist(), yticklabels=src.tolist())\n",
        "axes[0].set_title('Encoder Self-Attention (Layer 0, Head 0)', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('Key Position')\n",
        "axes[0].set_ylabel('Query Position')\n",
        "\n",
        "# Decoder self-attention\n",
        "sns.heatmap(decoder_self_attn, ax=axes[1], cmap='Greens', cbar=True,\n",
        "            xticklabels=tgt_input.tolist(), yticklabels=tgt_input.tolist())\n",
        "axes[1].set_title('Decoder Self-Attention (Layer 0, Head 0)', fontsize=12, fontweight='bold')\n",
        "axes[1].set_xlabel('Key Position')\n",
        "axes[1].set_ylabel('Query Position')\n",
        "\n",
        "# Decoder cross-attention\n",
        "sns.heatmap(decoder_cross_attn, ax=axes[2], cmap='Reds', cbar=True,\n",
        "            xticklabels=src.tolist(), yticklabels=tgt_input.tolist())\n",
        "axes[2].set_title('Decoder Cross-Attention (Layer 0, Head 0)', fontsize=12, fontweight='bold')\n",
        "axes[2].set_xlabel('Encoder Position (Source)')\n",
        "axes[2].set_ylabel('Decoder Position (Target)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Source sequence: {src.tolist()}\")\n",
        "print(f\"Target input: {tgt_input.tolist()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Visualize All Attention Heads\n",
        "\n",
        "Let's see how different attention heads capture different patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize all heads in the first encoder layer\n",
        "src, tgt = test_dataset[0]\n",
        "tgt_input = tgt[:-1]\n",
        "\n",
        "model.eval()\n",
        "src_tensor = src.unsqueeze(0).to(device)\n",
        "src_embedded = model.pos_encoder(\n",
        "    model.encoder_embedding(src_tensor) * math.sqrt(model.d_model)\n",
        ")\n",
        "encoder_output = src_embedded\n",
        "_, attn_weights = model.encoder_layers[0](encoder_output, None)\n",
        "\n",
        "# Extract all heads\n",
        "all_heads = attn_weights[0].cpu().numpy()  # Shape: (num_heads, seq_len, seq_len)\n",
        "\n",
        "# Create subplot for each head\n",
        "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for head_idx in range(NUM_HEADS):\n",
        "    sns.heatmap(all_heads[head_idx], ax=axes[head_idx], cmap='viridis', \n",
        "                cbar=True, xticklabels=src.tolist(), yticklabels=src.tolist())\n",
        "    axes[head_idx].set_title(f'Head {head_idx}', fontsize=10, fontweight='bold')\n",
        "    axes[head_idx].set_xlabel('Key')\n",
        "    axes[head_idx].set_ylabel('Query')\n",
        "\n",
        "plt.suptitle('All Attention Heads in Encoder Layer 0', fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 Visualize Attention Across Layers\n",
        "\n",
        "Let's see how attention patterns evolve through different layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get attention from all encoder layers\n",
        "src, tgt = test_dataset[0]\n",
        "tgt_input = tgt[:-1]\n",
        "\n",
        "model.eval()\n",
        "src_tensor = src.unsqueeze(0).to(device)\n",
        "src_embedded = model.pos_encoder(\n",
        "    model.encoder_embedding(src_tensor) * math.sqrt(model.d_model)\n",
        ")\n",
        "\n",
        "encoder_output = src_embedded\n",
        "all_layer_attentions = []\n",
        "\n",
        "for layer_idx, encoder_layer in enumerate(model.encoder_layers):\n",
        "    encoder_output, attn_weights = encoder_layer(encoder_output, None)\n",
        "    # Average across heads for cleaner visualization\n",
        "    avg_attn = attn_weights[0].mean(dim=0).cpu().numpy()\n",
        "    all_layer_attentions.append(avg_attn)\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, NUM_ENCODER_LAYERS, figsize=(6*NUM_ENCODER_LAYERS, 5))\n",
        "\n",
        "for layer_idx in range(NUM_ENCODER_LAYERS):\n",
        "    sns.heatmap(all_layer_attentions[layer_idx], ax=axes[layer_idx], \n",
        "                cmap='coolwarm', cbar=True, \n",
        "                xticklabels=src.tolist(), yticklabels=src.tolist())\n",
        "    axes[layer_idx].set_title(f'Encoder Layer {layer_idx}\\n(Avg across heads)', \n",
        "                              fontsize=12, fontweight='bold')\n",
        "    axes[layer_idx].set_xlabel('Key Position')\n",
        "    axes[layer_idx].set_ylabel('Query Position')\n",
        "\n",
        "plt.suptitle('Attention Patterns Across Encoder Layers', fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Key Takeaways\n",
        "\n",
        "### What We've Learned:\n",
        "\n",
        "1. **Attention Mechanism**: The core innovation that allows transformers to capture long-range dependencies without recurrence.\n",
        "\n",
        "2. **Multi-Head Attention**: Different heads learn to attend to different aspects of the input, providing rich representations.\n",
        "\n",
        "3. **Positional Encoding**: Critical for transformers since they don't have inherent notion of sequence order.\n",
        "\n",
        "4. **Layer Normalization & Residual Connections**: Enable training of deep transformer networks.\n",
        "\n",
        "5. **Encoder-Decoder Architecture**: The encoder processes the input, and the decoder generates the output while attending to encoder states.\n",
        "\n",
        "### Why Transformers are Powerful:\n",
        "\n",
        "- **Parallelization**: Unlike RNNs, all positions can be processed in parallel\n",
        "- **Long-range Dependencies**: Self-attention can directly connect distant positions\n",
        "- **Interpretability**: Attention weights provide insights into model decisions\n",
        "- **Scalability**: Can be scaled to billions of parameters (GPT, BERT, etc.)\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "- Experiment with different tasks (machine translation, text generation, etc.)\n",
        "- Try different architectures (encoder-only like BERT, decoder-only like GPT)\n",
        "- Explore pre-trained models and fine-tuning\n",
        "- Study advanced techniques (relative positional encoding, sparse attention, etc.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Additional Experiments\n",
        "\n",
        "Try modifying the following to deepen your understanding:\n",
        "\n",
        "1. **Change the task**: Modify the dataset to implement other sequence tasks\n",
        "2. **Adjust hyperparameters**: Experiment with different model sizes, learning rates, etc.\n",
        "3. **Add more layers**: See how depth affects performance\n",
        "4. **Analyze failure cases**: Look at examples where the model fails and understand why\n",
        "5. **Compare with RNNs**: Implement the same task with an RNN and compare performance\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
